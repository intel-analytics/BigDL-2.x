{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Traffic Forecasting with _AutoTS_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In telco, accurate forecast of KPIs (e.g. network traffic, utilizations, user experience, etc.) for communication networks ( 2G/3G/4G/5G/wired) can help predict network failures, allocate resource, or save energy. \n",
    "\n",
    "In this notebook, we demostrate a reference use case where we use the network traffic KPI(s) in the past to predict traffic KPI(s) in the future. We demostrate how to use `AutoTS` in project [Zouwu](https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/zouwu) to do time series forecasting in an automated and distributed way.\n",
    "\n",
    "For demonstration, we use the publicly available network traffic data repository maintained by the [WIDE project](http://mawi.wide.ad.jp/mawi/) and in particular, the network traffic traces aggregated every 2 hours (i.e. AverageRate in Mbps/Gbps and Total Bytes) in year 2018 and 2019 at the transit link of WIDE to the upstream ISP ([dataset link](http://mawi.wide.ad.jp/~agurim/dataset/)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section defines some helper functions to be used in the following procedures. You can refer to it later when they're used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:29.365279Z",
     "start_time": "2020-04-05T09:42:29.360456Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_drop_dates_and_len(df, allow_missing_num=3):\n",
    "    \"\"\"\n",
    "    Find missing values and get records to drop\n",
    "    \"\"\"\n",
    "    missing_num = df.total.isnull().astype(int).groupby(df.total.notnull().astype(int).cumsum()).sum()\n",
    "    drop_missing_num = missing_num[missing_num > allow_missing_num]\n",
    "    drop_datetimes = df.iloc[drop_missing_num.index].index\n",
    "    drop_len = drop_missing_num.values\n",
    "    return drop_datetimes, drop_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:29.567920Z",
     "start_time": "2020-04-05T09:42:29.561650Z"
    }
   },
   "outputs": [],
   "source": [
    "def rm_missing_weeks(start_dts, missing_lens, df):\n",
    "    \"\"\"\n",
    "    Drop weeks that contains more than 3 consecutive missing values.\n",
    "    If consecutive missing values across weeks, we remove all the weeks.\n",
    "    \"\"\" \n",
    "    for start_time, l in zip(start_dts, missing_lens):\n",
    "        start = start_time - pd.Timedelta(days=start_time.dayofweek)\n",
    "        start = start.replace(hour=0, minute=0, second=0)\n",
    "        start_week_end = start + pd.Timedelta(days=6)\n",
    "        start_week_end = start_week_end.replace(hour=22, minute=0, second=0)\n",
    "\n",
    "        end_time = start_time + l*pd.Timedelta(hours=2)\n",
    "        if start_week_end < end_time:\n",
    "            end = end_time + pd.Timedelta(days=6-end_time.dayofweek)\n",
    "            end = end.replace(hour=22, minute=0, second=0)\n",
    "        else:\n",
    "            end = start_week_end\n",
    "        df = df.drop(df[start:end].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:29.752013Z",
     "start_time": "2020-04-05T09:42:29.746678Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predicted values and actual values (for the test data)\n",
    "def plot_result(test_df, pred_df, dt_col=\"datetime\", value_col=\"AvgRate\", look_back=1):\n",
    "    # target column of dataframe is \"value\"\n",
    "    # past sequence length is 50\n",
    "    pred_value = pred_df[value_col].values\n",
    "    true_value = test_df[value_col].values[look_back:]\n",
    "    fig, axs = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "    axs.plot(pred_df[dt_col], pred_value, color='red', label='predicted values')\n",
    "    axs.plot(test_df[dt_col][look_back:], true_value, color='blue', label='actual values')\n",
    "    axs.set_title('the predicted values and actual values (for the test data)')\n",
    "\n",
    "    plt.xlabel(dt_col)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(value_col)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download raw dataset and load into dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we download the dataset and load it into a pandas dataframe. Steps are as below. \n",
    "\n",
    "* First, run the script `get_data.sh` to download the raw data. It will download the monthly aggregated traffic data in year 2018 and 2019 into `data` folder. The raw data contains aggregated network traffic (average MBPs and total bytes) as well as other metrics. \n",
    "\n",
    "* Second, run `extract_data.sh` to extract relavant traffic KPI's from raw data, i.e. `AvgRate` for average use rate, and `total` for total bytes. The script will extract the KPI's with timestamps into `data/data.csv`.\n",
    "\n",
    "* Finally, use pandas to load `data/data.csv` into a dataframe as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:30.934219Z",
     "start_time": "2020-04-05T09:42:30.324968Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:30.948562Z",
     "start_time": "2020-04-05T09:42:30.935369Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"data/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some example records of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:31.060336Z",
     "start_time": "2020-04-05T09:42:30.949794Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>AvgRate</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018/01/01 00:00:00</td>\n",
       "      <td>2018/01/01 02:00:00</td>\n",
       "      <td>306.23Mbps</td>\n",
       "      <td>275605455598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018/01/01 02:00:00</td>\n",
       "      <td>2018/01/01 04:00:00</td>\n",
       "      <td>285.03Mbps</td>\n",
       "      <td>256527692256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018/01/01 04:00:00</td>\n",
       "      <td>2018/01/01 06:00:00</td>\n",
       "      <td>247.39Mbps</td>\n",
       "      <td>222652190823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018/01/01 06:00:00</td>\n",
       "      <td>2018/01/01 08:00:00</td>\n",
       "      <td>211.55Mbps</td>\n",
       "      <td>190396029658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018/01/01 08:00:00</td>\n",
       "      <td>2018/01/01 10:00:00</td>\n",
       "      <td>234.82Mbps</td>\n",
       "      <td>211340468977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             StartTime              EndTime     AvgRate         total\n",
       "0  2018/01/01 00:00:00  2018/01/01 02:00:00  306.23Mbps  275605455598\n",
       "1  2018/01/01 02:00:00  2018/01/01 04:00:00  285.03Mbps  256527692256\n",
       "2  2018/01/01 04:00:00  2018/01/01 06:00:00  247.39Mbps  222652190823\n",
       "3  2018/01/01 06:00:00  2018/01/01 08:00:00  211.55Mbps  190396029658\n",
       "4  2018/01/01 08:00:00  2018/01/01 10:00:00  234.82Mbps  211340468977"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to do data cleaning and preprocessing on the raw data. Note that this part could vary for different dataset. \n",
    "\n",
    "For the network traffic data we're using, the processing contains 3 parts:\n",
    "1. Convert string datetime to TimeStamp\n",
    "2. Unify the measurement scale for `AvgRate` value - some uses Mbps, some uses Gbps \n",
    "3. Handle missing data (fill or drop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:31.439100Z",
     "start_time": "2020-04-05T09:42:31.433819Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.to_datetime(raw_df.StartTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:31.619691Z",
     "start_time": "2020-04-05T09:42:31.614336Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Mbps', 'Gbps'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can find 'AvgRate' is of two scales: 'Mbps' and 'Gbps' \n",
    "raw_df.AvgRate.str[-4:].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:31.817366Z",
     "start_time": "2020-04-05T09:42:31.810257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unify AvgRate value\n",
    "df['AvgRate'] = raw_df.AvgRate.apply(lambda x: float(x[:-4]) if x.endswith(\"Mbps\") else float(x[:-4]) * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:31.989842Z",
     "start_time": "2020-04-05T09:42:31.986685Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"total\"] = raw_df[\"total\"]\n",
    "df.set_index(\"StartTime\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:32.169108Z",
     "start_time": "2020-04-05T09:42:32.163918Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of n/a values:\n",
      "AvgRate    3\n",
      "total      3\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "full_idx = pd.date_range(start=df.index.min(), end=df.index.max(), freq='2H')\n",
    "df = df.reindex(full_idx)\n",
    "print(\"no. of n/a values:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we drop weeks with more than 3 consecutive missing values and fill other missing values remained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:32.545450Z",
     "start_time": "2020-04-05T09:42:32.534666Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_dts, drop_len = get_drop_dates_and_len(df)\n",
    "df = rm_missing_weeks(drop_dts, drop_len, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:32.715750Z",
     "start_time": "2020-04-05T09:42:32.713641Z"
    }
   },
   "outputs": [],
   "source": [
    "df.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:32.917402Z",
     "start_time": "2020-04-05T09:42:32.913676Z"
    }
   },
   "outputs": [],
   "source": [
    "# AutoTS requires input data frame with a datetime column\n",
    "df.index.name = \"datetime\"\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:33.110253Z",
     "start_time": "2020-04-05T09:42:33.101643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>AvgRate</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "      <td>306.23</td>\n",
       "      <td>2.756055e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01 02:00:00</td>\n",
       "      <td>285.03</td>\n",
       "      <td>2.565277e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-01 04:00:00</td>\n",
       "      <td>247.39</td>\n",
       "      <td>2.226522e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01 06:00:00</td>\n",
       "      <td>211.55</td>\n",
       "      <td>1.903960e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01 08:00:00</td>\n",
       "      <td>234.82</td>\n",
       "      <td>2.113405e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  AvgRate         total\n",
       "0 2018-01-01 00:00:00   306.23  2.756055e+11\n",
       "1 2018-01-01 02:00:00   285.03  2.565277e+11\n",
       "2 2018-01-01 04:00:00   247.39  2.226522e+11\n",
       "3 2018-01-01 06:00:00   211.55  1.903960e+11\n",
       "4 2018-01-01 08:00:00   234.82  2.113405e+11"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:33.292662Z",
     "start_time": "2020-04-05T09:42:33.280539Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AvgRate</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8760.000000</td>\n",
       "      <td>8.760000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>454.050030</td>\n",
       "      <td>4.084920e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>238.377074</td>\n",
       "      <td>2.146655e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>86.490000</td>\n",
       "      <td>8.641890e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>273.715000</td>\n",
       "      <td>2.459111e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>410.590000</td>\n",
       "      <td>3.694360e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>558.200000</td>\n",
       "      <td>5.023054e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1760.000000</td>\n",
       "      <td>1.585238e+12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           AvgRate         total\n",
       "count  8760.000000  8.760000e+03\n",
       "mean    454.050030  4.084920e+11\n",
       "std     238.377074  2.146655e+11\n",
       "min      86.490000  8.641890e+09\n",
       "25%     273.715000  2.459111e+11\n",
       "50%     410.590000  3.694360e+11\n",
       "75%     558.200000  5.023054e+11\n",
       "max    1760.000000  1.585238e+12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data to see how the KPI's look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:33.826495Z",
     "start_time": "2020-04-05T09:42:33.641397Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ax = df.plot(y='AvgRate',figsize=(12,5), title=\"AvgRate of network traffic data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:33.971770Z",
     "start_time": "2020-04-05T09:42:33.827516Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ax = df.plot(y='total',figsize=(12,5), title=\"total bytes of network traffic data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series forecasting with _AutoTS_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_AutoTS_ provides AutoML support for building end-to-end time series analysis pipelines (including automatic feature generation, model selection and hyperparameter tuning).\n",
    "\n",
    "The general workflow using automated training contains below two steps. \n",
    "   1. create a ```AutoTSTrainer``` to train a ```TSPipeline```, save it to file to use later or elsewhere if you wish.\n",
    "   2. use ```TSPipeline``` to do prediction, evaluation, and incremental fitting as well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to initialize RayOnSpark before using auto training (i.e. ```AutoTSTrainer```), and stop it after training finished. (Note RayOnSpark is not needed if you just use `TSPipeline` for inference, evaluation or incremental training.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:51.917822Z",
     "start_time": "2020-04-05T09:42:34.581511Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current pyspark location is : /home/ding/Downloads/spark-2.4.3-bin-hadoop2.7/python/pyspark/__init__.py\n",
      "Start to getOrCreate SparkContext\n",
      "Successfully got a SparkContext\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-29 16:03:58,851\tINFO resource_spec.py:212 -- Starting Ray with 21.14 GiB memory available for workers and up to 0.93 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
      "2020-09-29 16:03:59,082\tWARNING services.py:923 -- Redis failed to start, retrying now.\n",
      "2020-09-29 16:03:59,333\tINFO services.py:1165 -- View the Ray dashboard at \u001b[1m\u001b[32mlocalhost:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node_ip_address': '10.241.98.64', 'raylet_ip_address': '10.241.98.64', 'redis_address': '10.241.98.64:6379', 'object_store_address': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650/sockets/raylet', 'webui_url': 'localhost:8265', 'session_dir': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.241.98.64',\n",
       " 'raylet_ip_address': '10.241.98.64',\n",
       " 'redis_address': '10.241.98.64:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-09-29_16-03-58_850452_17650'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init RayOnSpark in local mode\n",
    "from zoo.common.nncontext import *\n",
    "sc = getOrCreateSparkContext()\n",
    "sc.stop()\n",
    "from zoo import init_spark_on_local\n",
    "from zoo.ray import RayContext\n",
    "sc = init_spark_on_local(cores=4, spark_log_level=\"INFO\")\n",
    "ray_ctx = RayContext(sc=sc, object_store_memory=\"1g\")\n",
    "ray_ctx.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize a `AutoTSTrainer`.\n",
    "* `dt_col`: the column specifying datetime.\n",
    "* `target_col`: target column to predict. Here, we take `AvgRate` KPI as an example.\n",
    "* `horizon` : num of steps to look forward. \n",
    "* `extra_feature_col`: a list of columns which are also included in input data frame as features except target column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:42:53.144776Z",
     "start_time": "2020-04-05T09:42:51.919760Z"
    }
   },
   "outputs": [],
   "source": [
    "from zoo.zouwu.autots.forecast import AutoTSTrainer\n",
    "\n",
    "trainer = AutoTSTrainer(dt_col=\"datetime\",\n",
    "                        target_col=\"AvgRate\",\n",
    "                        horizon=1,\n",
    "                        extra_features_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set some searching presets such as `look_back` which indicates the history time period we want to use for forecasting.\n",
    "lookback can be an int which it is a fixed values, or can be a tuple to indicate the range for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:44:10.228166Z",
     "start_time": "2020-04-05T09:44:10.225074Z"
    }
   },
   "outputs": [],
   "source": [
    "# look back in range from one week to 3 days to predict the next 2h.\n",
    "look_back = (36, 84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to split the data frame into train, validation and test data frame before training. You can use `train_val_test_split` as an easy way to finish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:44:56.311570Z",
     "start_time": "2020-04-05T09:44:56.298100Z"
    }
   },
   "outputs": [],
   "source": [
    "from zoo.automl.common.util import train_val_test_split\n",
    "train_df, val_df, test_df = train_val_test_split(df, \n",
    "                                                 val_ratio=0.1, \n",
    "                                                 test_ratio=0.1,\n",
    "                                                 look_back=look_back[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we fit on train data and validation data. \n",
    "\n",
    "You can use `recipe` to specify searching method as well as other searching presets such as stop criteria .etc. The `GridRandomRecipe` here is a recipe that combines grid search with random search to find the best set of parameters. For more details, please refer to analytics-zoo document [here](https://analytics-zoo.github.io/master/#APIGuide/AutoML/recipe/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:44:59.067809Z",
     "start_time": "2020-04-05T09:44:59.062375Z"
    }
   },
   "outputs": [],
   "source": [
    "from zoo.automl.config.recipe import LSTMGridRandomRecipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:03.575923Z",
     "start_time": "2020-04-05T09:45:31.368537Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.7/31.3 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/0 GPUs, 0.0/21.14 GiB heap, 0.0/0.63 GiB objects<br>Result logdir: /home/ding/ray_results/automl<br>Number of trials: 3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  dropout_2</th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  lstm_1_units</th><th style=\"text-align: right;\">  lstm_2_units</th><th style=\"text-align: right;\">  past_seq_len</th><th>selected_features                                                                                           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_func_74456_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.309508</td><td style=\"text-align: right;\">0.00853517</td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\">            16</td><td style=\"text-align: right;\">            38</td><td>[&quot;MONTH(datetime)&quot;, &quot;DAY(datetime)&quot;, &quot;IS_AWAKE(datetime)&quot;]                                                  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         50.5247</td></tr>\n",
       "<tr><td>train_func_74456_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.471959</td><td style=\"text-align: right;\">0.00333704</td><td style=\"text-align: right;\">           128</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            44</td><td>[&quot;MONTH(datetime)&quot;, &quot;WEEKDAY(datetime)&quot;, &quot;IS_BUSY_HOURS(datetime)&quot;, &quot;IS_WEEKEND(datetime)&quot;]                 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         69.7632</td></tr>\n",
       "<tr><td>train_func_74456_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">   0.402179</td><td style=\"text-align: right;\">0.00841629</td><td style=\"text-align: right;\">            32</td><td style=\"text-align: right;\">            64</td><td style=\"text-align: right;\">            50</td><td>[&quot;HOUR(datetime)&quot;, &quot;DAY(datetime)&quot;, &quot;IS_BUSY_HOURS(datetime)&quot;, &quot;IS_AWAKE(datetime)&quot;, &quot;IS_WEEKEND(datetime)&quot;]</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         49.5204</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best log dir is  /home/ding/ray_results/automl/train_func_0_batch_size=64,dropout_2=0.30951,lr=0.0085352,lstm_1_units=16,lstm_2_units=16,past_seq_len=38,selected_features=[\"MONT_2020-09-29_16-06-49k6nmgnt8\n",
      "The best configurations are:\n",
      "selected_features : [\"MONTH(datetime)\", \"DAY(datetime)\", \"IS_AWAKE(datetime)\"]\n",
      "model : LSTM\n",
      "lstm_1_units : 16\n",
      "dropout_1 : 0.2\n",
      "lstm_2_units : 16\n",
      "dropout_2 : 0.3095076432762375\n",
      "lr : 0.008535167820304366\n",
      "batch_size : 64\n",
      "epochs : 1\n",
      "past_seq_len : 38\n",
      "WARNING:tensorflow:From /home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:OMP_NUM_THREADS is no longer used by the default Keras config. To configure the number of threads, use tf.config.threading APIs.\n",
      "WARNING:tensorflow:From /home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "CPU times: user 4.85 s, sys: 504 ms, total: 5.35 s\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ts_pipeline = trainer.fit(train_df, val_df, \n",
    "                          recipe=LSTMGridRandomRecipe(\n",
    "                              num_rand_samples=1,\n",
    "                              epochs=1,\n",
    "                              look_back=look_back, \n",
    "                              batch_size=[64]),\n",
    "                          metric=\"mse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a `TSPipeline` after training. Let's print the hyper paramters selected.\n",
    "Note that `past_seq_len` is the lookback value that is automatically chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:10.610297Z",
     "start_time": "2020-04-05T09:48:10.607153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean': [470.45753424657534,\n",
       "  5.633561643835616,\n",
       "  15.558219178082192,\n",
       "  0.8333333333333334],\n",
       " 'scale': [237.54721125499952,\n",
       "  3.2248983714954504,\n",
       "  8.827884346441833,\n",
       "  0.3726779962499649],\n",
       " 'future_seq_len': 1,\n",
       " 'dt_col': 'datetime',\n",
       " 'target_col': 'AvgRate',\n",
       " 'extra_features_col': None,\n",
       " 'drop_missing': True,\n",
       " 'model': 'LSTM',\n",
       " 'metric': 'mean_squared_error',\n",
       " 'batch_size': 64,\n",
       " 'selected_features': '[\"MONTH(datetime)\", \"DAY(datetime)\", \"IS_AWAKE(datetime)\"]',\n",
       " 'lstm_1_units': 16,\n",
       " 'dropout_1': 0.2,\n",
       " 'lstm_2_units': 16,\n",
       " 'dropout_2': 0.3095076432762375,\n",
       " 'lr': 0.008535167820304366,\n",
       " 'epochs': 1,\n",
       " 'past_seq_len': 38}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_pipeline.internal.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to do prediction, evaluation or incremental fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:18.212911Z",
     "start_time": "2020-04-05T09:48:17.672724Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The reset parameter is False but there is no n_features_in_ attribute. Is this estimator fitted?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-050f12b243b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/spark-b000844b-7515-4241-a8e2-462c8c53b252/userFiles-c3f65cb2-18ee-405e-a187-c0d3a60c5c5d/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/zouwu/autots/forecast.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_df)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_with_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/tmp/spark-b000844b-7515-4241-a8e2-462c8c53b252/userFiles-c3f65cb2-18ee-405e-a187-c0d3a60c5c5d/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/automl/pipeline/time_sequence.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input_df)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \"\"\"\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0my_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-b000844b-7515-4241-a8e2-462c8c53b252/userFiles-c3f65cb2-18ee-405e-a187-c0d3a60c5c5d/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/automl/feature/time_sequence.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, input_df, is_train)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0moutput_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0moutput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/spark-b000844b-7515-4241-a8e2-462c8c53b252/userFiles-c3f65cb2-18ee-405e-a187-c0d3a60c5c5d/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/automl/feature/time_sequence.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, input_df, mode)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mfeature_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# select and standardize data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mdata_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             (x, y) = self._roll_train(data_n,\n",
      "\u001b[0;32m/tmp/spark-b000844b-7515-4241-a8e2-462c8c53b252/userFiles-c3f65cb2-18ee-405e-a187-c0d3a60c5c5d/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/automl/feature/time_sequence.py\u001b[0m in \u001b[0;36m_scale\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \"\"\"\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mnp_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m         \u001b[0mdata_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_py36/lib/python3.6/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m    792\u001b[0m                                 \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m                                 force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_py36/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ensure_2d'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_py36/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_features_in_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 raise RuntimeError(\n\u001b[0;32m--> 373\u001b[0;31m                     \u001b[0;34m\"The reset parameter is False but there is no \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                     \u001b[0;34m\"n_features_in_ attribute. Is this estimator fitted?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The reset parameter is False but there is no n_features_in_ attribute. Is this estimator fitted?"
     ]
    }
   ],
   "source": [
    "pred_df = ts_pipeline.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot actual and prediction values for `AvgRate` KPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:20.239860Z",
     "start_time": "2020-04-05T09:48:20.074644Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the predicted values and actual values\n",
    "plot_result(test_df, pred_df, dt_col=\"datetime\", value_col=\"AvgRate\", look_back=ts_pipeline.internal.config['past_seq_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean square error and the symetric mean absolute percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:24.353282Z",
     "start_time": "2020-04-05T09:48:23.926760Z"
    }
   },
   "outputs": [],
   "source": [
    "mse, smape = ts_pipeline.evaluate(test_df, metrics=[\"mse\", \"smape\"])\n",
    "print(\"Evaluate: the mean square error is\", mse)\n",
    "print(\"Evaluate: the smape value is\", smape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the pipeline to file and reload it to do incremental fitting or others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:30.202808Z",
     "start_time": "2020-04-05T09:48:30.023909Z"
    }
   },
   "outputs": [],
   "source": [
    "# save pipeline file\n",
    "my_ppl_file_path = ts_pipeline.save(\"/tmp/saved_pipeline/my.ppl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop RayOnSpark after auto training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:33.286478Z",
     "start_time": "2020-04-05T09:48:32.916403Z"
    }
   },
   "outputs": [],
   "source": [
    "# stop\n",
    "ray_ctx.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we demonstrate how to do incremental fitting with your saved pipeline file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First load saved pipeline file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:48:37.523732Z",
     "start_time": "2020-04-05T09:48:35.745574Z"
    }
   },
   "outputs": [],
   "source": [
    "# load file\n",
    "from zoo.zouwu.autots.forecast import TSPipeline\n",
    "loaded_ppl = TSPipeline.load(my_ppl_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then do incremental fitting with `TSPipeline.fit()`.We use validation data frame as additional data for demonstration. You can use your new data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:50:10.820564Z",
     "start_time": "2020-04-05T09:50:09.321120Z"
    }
   },
   "outputs": [],
   "source": [
    "# we use validation data frame as additional data for demonstration. \n",
    "loaded_ppl.fit(val_df, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict and plot the result after incremental fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:50:13.535064Z",
     "start_time": "2020-04-05T09:50:13.007472Z"
    }
   },
   "outputs": [],
   "source": [
    "# predict results of test_df\n",
    "new_pred_df = loaded_ppl.predict(test_df)\n",
    "plot_result(test_df, new_pred_df, look_back=loaded_ppl.internal.config['past_seq_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean square error and the symetric mean absolute percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-05T09:50:16.412656Z",
     "start_time": "2020-04-05T09:50:15.986206Z"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate test_df\n",
    "mse, smape = loaded_ppl.evaluate(test_df, metrics=[\"mse\", \"smape\"])\n",
    "print(\"Evaluate: the mean square error is\", mse)\n",
    "print(\"Evaluate: the smape value is\", smape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
