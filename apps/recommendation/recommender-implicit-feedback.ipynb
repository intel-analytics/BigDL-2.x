{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation with Implicit Feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demostrate how to build neural network recommendation system with implicit feedback, which indirectly reflects usersâ€™ preference through behaviours like watching videos, purchasing products and clicking items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start BigDL engine and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.intel.analytics.bigdl._\n",
    "import com.intel.analytics.bigdl.nn._\n",
    "import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.optim._\n",
    "import com.intel.analytics.bigdl.utils.Engine\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.ml.{DLClassifier, DLModel}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val conf = Engine.createSparkConf()\n",
    "val spark = SparkSession.builder().master(\"local[8]\").appName(\"RecommendationImplicitExample\").config(conf).getOrCreate()\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "Engine.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data and check data quality. Here, we assume it is downloaded from [link](https://grouplens.org/datasets/movielens/1m/) and put it into directory of \"/tmp/movielens/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "// mvpath is the location of downloaded data\n",
    "val mvpath = \"/tmp/movielens/ml-1m\"\n",
    "\n",
    "def getDataDF = {\n",
    "    val indexedDF = spark.read.text(mvpath + \"/ratings.dat\").as[String]\n",
    "      .map(x => {\n",
    "        val data: Array[Double] = x.split(\"::\").map(n => n.toDouble)\n",
    "        (data(0), data(1), data(2))\n",
    "      })\n",
    "      .toDF(\"userIdIndex\",\"itemIdIndex\",\"label\")\n",
    "    \n",
    "    val minMaxRow = indexedDF.agg(min(\"userIdIndex\"), max(\"userIdIndex\"), min(\"itemIdIndex\"), max(\"itemIdIndex\")).collect()(0)\n",
    "\n",
    "    val minUserId = minMaxRow.getDouble(0)\n",
    "    val userCount = minMaxRow.getDouble(1)\n",
    "    val minMovieId = minMaxRow.getDouble(2)\n",
    "    val itemCount = minMaxRow.getDouble(3)\n",
    "    (indexedDF, userCount, itemCount)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|userIdIndex|itemIdIndex|label|\n",
      "+-----------+-----------+-----+\n",
      "|        1.0|     1193.0|  5.0|\n",
      "|        1.0|      661.0|  3.0|\n",
      "|        1.0|      914.0|  3.0|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "userCount: 6040.0\n",
      "itemCount: 3952.0\n"
     ]
    }
   ],
   "source": [
    "val (indexedDF, userCount, itemCount) = getDataDF\n",
    "indexedDF.show(3)\n",
    "println(\"userCount: \" + userCount + \"\\nitemCount: \" + itemCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We focus on impicit feedback, the user-item interaction is defined as 1 if interaction, 0 otherwise.  Ratings are all trasformed into 1.0 and negative samples are added by randomly sampling from the whole user and item space, then prepare features into label points required by DLClassifer and DLModel.\n",
    "* Then split data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import org.apache.spark.ml.feature.{LabeledPoint, StringIndexer}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "/* Negative samples are needed in this use case. \n",
    " addNegativeSample is defined to add negative samples by randomly sampling from the whole user and item space.\n",
    "*/\n",
    "def addNegativeSample(indexedDF: DataFrame) = {\n",
    "    val row = indexedDF.agg(max(\"userIdIndex\"), max(\"itemIdIndex\")).head\n",
    "    val (userCount, itemCount) = (row.getAs[Double](0).toInt, row.getAs[Double](1).toInt)\n",
    "\n",
    "    val sampleDict = indexedDF.rdd.map(row => row(0) + \",\" + row(1)).collect().toSet\n",
    "    val numberRecords = 1 * indexedDF.count\n",
    "\n",
    "    val ran = new Random(seed = 42L)\n",
    "    val negativeSampleDF = indexedDF.sparkSession.sparkContext\n",
    "      .parallelize(0 to numberRecords.toInt)\n",
    "      .map(x => {\n",
    "        val uid = Math.max(ran.nextInt(userCount), 1)\n",
    "        val iid = Math.max(ran.nextInt(itemCount), 1)\n",
    "        (uid, iid)\n",
    "      })\n",
    "      .filter(x => !sampleDict.contains(x._1 + \",\" + x._2)).distinct()\n",
    "      .map(x => (x._1, x._2, 0.0))\n",
    "      .toDF(\"userIdIndex\", \"itemIdIndex\", \"label\")\n",
    "\n",
    "    indexedDF.union(negativeSampleDF)\n",
    "  }\n",
    "\n",
    "// To tranform the dataframe of sparse features into label points\n",
    "val df2LP: (DataFrame) => DataFrame = df => {\n",
    "    import df.sparkSession.implicits._\n",
    "    df.select(\"userIdIndex\", \"itemIdIndex\", \"label\").rdd.map { r =>\n",
    "      val f = Vectors.dense(r.getDouble(0), r.getDouble(1))\n",
    "      require(f.toArray.take(2).forall(_ >= 0))\n",
    "      val l = r.getDouble(2)\n",
    "      LabeledPoint(l, f)\n",
    "    }.toDF().orderBy(rand()).cache()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 183:====================>                                   (5 + 8) / 14]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val add1 = udf((num: Double) => num + 1)\n",
    "\n",
    "val dataWithNegative = addNegativeSample(indexedDF.withColumn(\"label\", lit(1.0d))).withColumn(\"label\", add1(col(\"label\")))\n",
    "val dataInLP: DataFrame = df2LP(dataWithNegative)\n",
    "\n",
    "val Array(trainingDF, validationDF) = dataInLP.randomSplit(Array(0.8, 0.2), seed = 1L)\n",
    "\n",
    "trainingDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we show how to build a Multi-Layer Perceptron (MLP). A ModelParam class is defined to easily change model architecture. \n",
    "* The buttom layer is input layer, then it is embedding layer, which projects the sparse representation to a dense vector. In BigDL, we can use LookUpTable together with Select to create embedding layers, the user (item) embedding has input size of userCount(itemCount), output size of userEmbed (itemEmbed). At last, embedding layers are fed into a multi-layer neural architecture (midLayers). Eventually, a layer of LogSoftMax is added at the end. \n",
    "* Please refer to([BigDL programming guide](https://bigdl-project.github.io/master/#ProgrammingGuide/Model/Functional/)) for more details about functional API, and Nerual Collaborative filtering ([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)) for details about the architeture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class ModelParam(userEmbed: Int = 20,\n",
    "                      itemEmbed: Int = 20,\n",
    "                      mfEmbed: Int = 20,\n",
    "                      midLayers: Array[Int] = Array(40, 20, 10),\n",
    "                      labels: Int = 2){\n",
    "  override def toString: String = {\n",
    "    \"userEmbed =\" + userEmbed + \"\\n\" +\n",
    "    \" itemEmbed = \" + itemEmbed + \"\\n\" +\n",
    "    \" mfEmbed = \" + mfEmbed + \"\\n\" +\n",
    "    \" midLayer = \" + midLayers.mkString(\"|\") + \"\\n\" +\n",
    "    \" labels = \" + labels\n",
    "  }\n",
    "}\n",
    "\n",
    "class Model(modelParam: ModelParam) {\n",
    "    import com.intel.analytics.bigdl.nn.Graph.ModuleNode\n",
    "    import com.intel.analytics.bigdl.nn.{Graph, _}\n",
    "    import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "    import com.intel.analytics.bigdl.tensor.Tensor\n",
    "    import com.intel.analytics.bigdl.nn._\n",
    "    \n",
    "  def this() = {\n",
    "    this(ModelParam())\n",
    "  }\n",
    "\n",
    "  def mlp(userCount: Int, itemCount: Int) = {\n",
    "\n",
    "    println(modelParam )\n",
    "\n",
    "    val input = Identity().inputs()\n",
    "    val select1: ModuleNode[Float] = Select(2, 1).inputs(input)\n",
    "    val select2: ModuleNode[Float] = Select(2, 2).inputs(input)\n",
    "\n",
    "    val userTable = LookupTable(userCount, modelParam.userEmbed)\n",
    "    val itemTable = LookupTable(itemCount, modelParam.itemEmbed)\n",
    "    userTable.setWeightsBias(Array(Tensor[Float](userCount, modelParam.userEmbed).randn(0, 0.1)))\n",
    "    itemTable.setWeightsBias(Array(Tensor[Float](itemCount, modelParam.itemEmbed).randn(0, 0.1)))\n",
    "\n",
    "    val userTableInput = userTable.inputs(select1)\n",
    "    val itemTableInput = itemTable.inputs(select2)\n",
    "\n",
    "    val embeddedLayer = JoinTable(2, 0).inputs(userTableInput, itemTableInput)\n",
    "\n",
    "    val linear1: ModuleNode[Float] = Linear(modelParam.itemEmbed + modelParam.userEmbed,\n",
    "      modelParam.midLayers(0)).inputs(embeddedLayer)\n",
    "\n",
    "    val midLayer = buildMlpModuleNode(linear1, 1, modelParam.midLayers)\n",
    "\n",
    "    val reluLast = ReLU().inputs(midLayer)\n",
    "    val last: ModuleNode[Float] = Linear(modelParam.midLayers.last, modelParam.labels).inputs(reluLast)\n",
    "\n",
    "    val output = if (modelParam.labels >= 2) LogSoftMax().inputs(last) else Sigmoid().inputs(last)\n",
    "\n",
    "    Graph(input, output)\n",
    "  }\n",
    "\n",
    "  private def buildMlpModuleNode(linear: ModuleNode[Float], midLayerIndex: Int, midLayers: Array[Int]): ModuleNode[Float] = {\n",
    "\n",
    "    if (midLayerIndex >= midLayers.length) {\n",
    "      linear\n",
    "    } else {\n",
    "      val relu = ReLU().inputs(linear)\n",
    "      val l = Linear(midLayers(midLayerIndex - 1), midLayers(midLayerIndex)).inputs(relu)\n",
    "      buildMlpModuleNode(l, midLayerIndex + 1, midLayers)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userEmbed =20\n",
      " itemEmbed = 20\n",
      " mfEmbed = 20\n",
      " midLayer = 20|10\n",
      " labels = 2\n"
     ]
    }
   ],
   "source": [
    "val modelParam = ModelParam(userEmbed = 20,\n",
    "                            itemEmbed = 20,\n",
    "                            midLayers = Array(20,10),\n",
    "                            labels = 2)\n",
    "val recModel = new Model(modelParam)\n",
    "val model = recModel.mlp(userCount.toInt, itemCount.toInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using DLclassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BigDL provides DLEstimator and DLClassifier for users with Apache Spark MLlib experience, which provides high level API for training a BigDL Model with the Apache Spark Estimator/ Transfomer pattern, thus users can conveniently fit BigDL into a ML pipeline. Please refer to [BigDL guide](https://bigdl-project.github.io/master/#ProgrammingGuide/MLPipeline/#overview) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# val criterion = ClassNLLCriterion()\n",
    "val dlc = new DLClassifier(model, criterion, Array(2)).setBatchSize(1000).setOptimMethod(new Adam()).setLearningRate(1e-1).setLearningRateDecay(1e-6).setMaxEpoch(3)\n",
    "\n",
    "val time1 = System.nanoTime()\n",
    "\n",
    "val dlModel: DLModel[Float] = dlc.fit(trainingDF)\n",
    "trainingDF.unpersist()\n",
    "\n",
    "val time2 = System.nanoTime()\n",
    "\n",
    "println(\"training time(s):  \" + (time2-time1)*(1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recommendation system could be evaluated using different metrics, here we show example of using traditional area under the curve, precision and recall. Metrics based on different customer's use cases are prefered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "object Evaluation {\n",
    "  \n",
    "  def toDecimal(n: Int) = {\n",
    "    (arg: Double) => BigDecimal(arg).setScale(n, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "   }\n",
    "    \n",
    "  def evaluate(evaluateDF: DataFrame) = {\n",
    "    val binaryEva = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\")\n",
    "    val out1 = binaryEva.evaluate(evaluateDF)\n",
    "    println(\"AUROC: \" + toDecimal(3)(out1))\n",
    "\n",
    "    val multiEva = new MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\")\n",
    "    val out2 = multiEva.evaluate(evaluateDF)\n",
    "    println(\"precision: \" + toDecimal(3)(out2))\n",
    "\n",
    "    val multiEva2 = new MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\")\n",
    "    val out3 = multiEva2.evaluate(evaluateDF)\n",
    "    println(\"recall: \" + toDecimal(3)(out3))\n",
    "\n",
    "    Seq(out1, out2, out3).map(x=> toDecimal(3)(x))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+\n",
      "|label|      features|prediction|\n",
      "+-----+--------------+----------+\n",
      "|  1.0|  [81.0,981.0]|       1.0|\n",
      "|  1.0|[115.0,2218.0]|       1.0|\n",
      "|  1.0|[162.0,2706.0]|       2.0|\n",
      "+-----+--------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "[Stage 65148:=================================================> (196 + 4) / 200]AUROC: 0.731\n",
      "precision: 0.908\n",
      "recall: 0.916\n",
      "userEmbed =20\n",
      " itemEmbed = 20\n",
      " mfEmbed = 20\n",
      " midLayer = 20|10\n",
      " labels = 2\n",
      "0.731 | 0.908 | 0.916\n",
      "\n",
      "prediction and evaluation time(s):  122.54388534700001\n"
     ]
    }
   ],
   "source": [
    "val predictions = dlModel.setBatchSize(1).transform(validationDF)\n",
    "predictions.show(3)\n",
    "predictions.cache()\n",
    "\n",
    "val toZero = udf { d: Double =>\n",
    "    if (d > 1) 1.0 else 0.0\n",
    " }\n",
    "val res = Evaluation.evaluate(predictions.withColumn(\"label\", toZero(col(\"label\"))).withColumn(\"prediction\", toZero(col(\"prediction\"))))\n",
    "val time3 = System.nanoTime()\n",
    "val resStr = modelParam +\"\\n\" + res.mkString(\" | \") +\"\\n\"\n",
    "\n",
    "println(resStr)\n",
    "println(\"prediction and evaluation time(s):  \" + (time3-time2)*(1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example to show recommendations for users, recommendations for item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val y = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val z = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val x = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val x = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
