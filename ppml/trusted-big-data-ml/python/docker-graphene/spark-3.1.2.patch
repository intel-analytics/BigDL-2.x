diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
index fa86da9ae9..ceb5dfbe6a 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala
@@ -648,10 +648,20 @@ private[spark] class SparkSubmit extends Logging {
         confKey = DRIVER_SUPERVISE.key),
       OptionAssigner(args.ivyRepoPath, STANDALONE, CLUSTER, confKey = "spark.jars.ivy"),

+      // SGX related
+      OptionAssigner(args.sgxEnabled.toString, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_ENABLED.key),
+      OptionAssigner(args.sgxMem, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_MEM_SIZE.key),
+      OptionAssigner(args.sgxJvmMem, KUBERNETES, ALL_DEPLOY_MODES,
+        confKey = SGX_JVM_MEM_SIZE.key),
+
       // An internal option used only for spark-shell to add user jars to repl's classloader,
       // previously it uses "spark.jars" or "spark.yarn.dist.jars" which now may be pointed to
       // remote jars, so adding a new option to only specify local jars for spark-shell internally.
       OptionAssigner(localJars, ALL_CLUSTER_MGRS, CLIENT, confKey = "spark.repl.local.jars")
+
+
     )

     // In client mode, launch the application main class directly
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
index 9da1a73bba..b09ceb6a18 100644
--- a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala
@@ -76,6 +76,11 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
   var keytab: String = null
   private var dynamicAllocationEnabled: Boolean = false

+  // sgx
+  var sgxEnabled = false
+  var sgxMem: String = null
+  var sgxJvmMem: String = null
+
   // Standalone cluster mode only
   var supervise: Boolean = false
   var driverCores: String = null
@@ -210,6 +215,12 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     dynamicAllocationEnabled =
       sparkProperties.get(DYN_ALLOCATION_ENABLED.key).exists("true".equalsIgnoreCase)

+    sgxEnabled = sparkProperties.get(config.SGX_ENABLED.key).exists("true".equalsIgnoreCase)
+    sgxMem = Option(sgxMem)
+      .getOrElse(sparkProperties.get(config.SGX_MEM_SIZE.key).orNull)
+    sgxJvmMem = Option(sgxJvmMem)
+      .getOrElse(sparkProperties.get(config.SGX_JVM_MEM_SIZE.key).orNull)
+
     // Global defaults. These should be keep to minimum to avoid confusing behavior.
     master = Option(master).getOrElse("local[*]")

@@ -321,6 +332,9 @@ private[deploy] class SparkSubmitArguments(args: Seq[String], env: Map[String, S
     |  packagesExclusions      $packagesExclusions
     |  repositories            $repositories
     |  verbose                 $verbose
+    |  sgxEnabled              $sgxEnabled
+    |  sgxMem                  $sgxMem
+    |  sgxJvmMem               $sgxJvmMem
     |
     |Spark properties used, including those specified through
     | --conf and those from the properties file $propertiesFile:
diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala
index 3daa9f5362..9758246e94 100644
--- a/core/src/main/scala/org/apache/spark/internal/config/package.scala
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala
@@ -2023,4 +2023,25 @@ package object config {
       .version("3.1.0")
       .doubleConf
       .createWithDefault(5)
+
+  private[spark] val SGX_ENABLED =
+    ConfigBuilder("spark.kubernetes.sgx.enabled")
+      .doc("If set to true, spark executors on kubernetes will run in sgx.")
+      .version("3.1.2")
+      .booleanConf
+      .createWithDefault(false)
+
+  private[spark] val SGX_MEM_SIZE =
+    ConfigBuilder("spark.kubernetes.sgx.mem")
+      .doc("Amount of memory to use for the sgx initialized, in GiB unless otherwise specified.")
+      .version("3.1.2")
+      .bytesConf(ByteUnit.GiB)
+      .createWithDefaultString("16g")
+
+  private[spark] val SGX_JVM_MEM_SIZE =
+    ConfigBuilder("spark.kubernetes.sgx.jvm.mem")
+      .doc("Amount of memory to use for the jvm run in sgx, in GiB unless otherwise specified.")
+      .version("3.1.2")
+      .bytesConf(ByteUnit.GiB)
+      .createWithDefaultString("16g")
 }
diff --git a/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala b/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
index 1ebd8bd89f..df72ed2dbc 100644
--- a/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
+++ b/core/src/main/scala/org/apache/spark/resource/ResourceProfile.scala
@@ -353,9 +353,12 @@ object ResourceProfile extends Logging {
     }
     val customResourceNames = execReq.map(_.id.resourceName).toSet
     val customResources = ereqs.requests.filter(v => customResourceNames.contains(v._1))
+    val sgxEnabled = conf.get(SGX_ENABLED)
+    val sgxMem = conf.get(SGX_MEM_SIZE)
+    val sgxJvmMem = conf.get(SGX_JVM_MEM_SIZE)
     defaultProfileExecutorResources =
       Some(DefaultProfileExecutorResources(cores, memory, offheapMem, pysparkMem,
-        overheadMem, customResources))
+        overheadMem, customResources, sgxEnabled, sgxMem, sgxJvmMem))
     ereqs.requests
   }

@@ -408,7 +411,10 @@ object ResourceProfile extends Logging {
       pysparkMemoryMiB: Long,
       memoryOverheadMiB: Long,
       totalMemMiB: Long,
-      customResources: Map[String, ExecutorResourceRequest])
+      customResources: Map[String, ExecutorResourceRequest],
+      sgxEnabled: Boolean,
+      sgxMemGiB: Long,
+      sgxJvmMemGiB: Long)

   private[spark] case class DefaultProfileExecutorResources(
       cores: Int,
@@ -416,7 +422,10 @@ object ResourceProfile extends Logging {
       memoryOffHeapMiB: Long,
       pysparkMemoryMiB: Option[Long],
       memoryOverheadMiB: Option[Long],
-      customResources: Map[String, ExecutorResourceRequest])
+      customResources: Map[String, ExecutorResourceRequest],
+      sgxEnabled: Boolean,
+      sgxMemGiB: Long,
+      sgxJvmMemGiB: Long)

   private[spark] def calculateOverHeadMemory(
       overHeadMemFromConf: Option[Long],
@@ -448,6 +457,7 @@ object ResourceProfile extends Logging {
     var memoryOverheadMiB = calculateOverHeadMemory(defaultResources.memoryOverheadMiB,
       executorMemoryMiB, overheadFactor)

+
     val finalCustomResources = if (rpId != DEFAULT_RESOURCE_PROFILE_ID) {
       val customResources = new mutable.HashMap[String, ExecutorResourceRequest]
       execResources.foreach { case (r, execReq) =>
@@ -482,8 +492,12 @@ object ResourceProfile extends Logging {
     }
     val totalMemMiB =
       (executorMemoryMiB + memoryOverheadMiB + memoryOffHeapMiB + pysparkMemToUseMiB)
+    val sgxEnabled = defaultResources.sgxEnabled
+    val sgxMemGiB = defaultResources.sgxMemGiB
+    val sgxJvmMemGiB = defaultResources.sgxJvmMemGiB
     ExecutorResourcesOrDefaults(cores, executorMemoryMiB, memoryOffHeapMiB,
-      pysparkMemToUseMiB, memoryOverheadMiB, totalMemMiB, finalCustomResources)
+      pysparkMemToUseMiB, memoryOverheadMiB, totalMemMiB, finalCustomResources,
+      sgxEnabled, sgxMemGiB, sgxJvmMemGiB)
   }

   private[spark] val PYSPARK_MEMORY_LOCAL_PROPERTY = "resource.pyspark.memory"
diff --git a/core/src/main/scala/org/apache/spark/util/Utils.scala b/core/src/main/scala/org/apache/spark/util/Utils.scala
index 1643aa68cd..ceeaf9f4e4 100644
--- a/core/src/main/scala/org/apache/spark/util/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala
@@ -560,7 +560,10 @@ private[spark] object Utils extends Logging {
       }
     }
     // Make the file executable - That's necessary for scripts
-    FileUtil.chmod(targetFile.getAbsolutePath, "a+x")
+    // scalastyle:off
+    println("INFO fork chmod is forbidden !!!" + targetFile.getAbsolutePath)
+    // scalastyle:on
+    // FileUtil.chmod(targetFile.getAbsolutePath, "a+x")

     // Windows does not grant read permission by default to non-admin users
     // Add read permission to owner explicitly
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
index 543ca12594..9dfb3697f3 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/Constants.scala
@@ -65,6 +65,9 @@ private[spark] object Constants {
   val ENV_SPARK_CONF_DIR = "SPARK_CONF_DIR"
   val ENV_SPARK_USER = "SPARK_USER"
   val ENV_RESOURCE_PROFILE_ID = "SPARK_RESOURCE_PROFILE_ID"
+  val ENV_SGX_ENABLED = "SGX_ENABLED"
+  val ENV_SGX_MEM_SIZE = "SGX_MEM_SIZE"
+  val ENV_SGX_JVM_MEM_SIZE = "SGX_JVM_MEM_SIZE"
   // Spark app configs for containers
   val SPARK_CONF_VOLUME_DRIVER = "spark-conf-volume-driver"
   val SPARK_CONF_VOLUME_EXEC = "spark-conf-volume-exec"
diff --git a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
index a0a17cecf9..6040a421c1 100644
--- a/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
+++ b/resource-managers/kubernetes/core/src/main/scala/org/apache/spark/deploy/k8s/features/BasicExecutorFeatureStep.scala
@@ -121,7 +121,10 @@ private[spark] class BasicExecutorFeatureStep(
           // This is to set the SPARK_CONF_DIR to be /opt/spark/conf
           (ENV_SPARK_CONF_DIR, SPARK_CONF_DIR_INTERNAL),
           (ENV_EXECUTOR_ID, kubernetesConf.executorId),
-          (ENV_RESOURCE_PROFILE_ID, resourceProfile.id.toString)
+          (ENV_RESOURCE_PROFILE_ID, resourceProfile.id.toString),
+          (ENV_SGX_ENABLED, execResources.sgxEnabled.toString),
+          (ENV_SGX_MEM_SIZE, execResources.sgxMemGiB + "G"),
+          (ENV_SGX_JVM_MEM_SIZE, execResources.sgxJvmMemGiB + "G")
         ) ++ kubernetesConf.environment).map { case (k, v) =>
           new EnvVarBuilder()
             .withName(k)
