# Distributed Training and Inference

---

**Orca `Estimator` provides sklearn-style APIs for transparently distributed model training and inference** 

### **1. Estimator**

To perform distributed training and inference, the user can first create an Orca `Estimator` from any standard (single-node) TensorFlow, Kera or PyTorch model, and then call `Estimator.fit` or `Estimator.predict`  methods (using the [data-parallel processing pipeline](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) as input).

Under the hood, the Orca `Estimator` will replicate the model on each node in the cluster, feed the data partition (generated by the data-parallel processing pipeline) on each node to the local model replica, and synchronize model parameters using various *backend* technologies (such as *Horovod*, `tf.distribute.MirroredStrategy`, `torch.distributed`, or the parameter sync layer in [*BigDL*](https://bigdl-project.github.io/master/)).

### **2. TensorFlow/Keras Estimator**

#### **2.1 TensorFlow 1.15 and Keras 2.3**

There two ways to create a TensorFlow `Estimator` for users of TensorFlow 1.15. One is to create the Estimator from a low level graph and another from a keras model. Examples are as follows:

Graph API:
```python
# define inputs to the graph
images = tf.placeholder(dtype=tf.float32, shape=(None, 28, 28, 1))
labels = tf.placeholder(dtype=tf.int32, shape=(None,))

# define the network and loss
logits = lenet(images)
loss = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels))

# define a metric
acc = accuracy(logits, labels)

# create an estimator using endpoints of the graph
est = Estimator.from_graph(inputs=images,
                           outputs=logits,
                           labels=labels,
                           loss=loss,
                           optimizer=tf.train.AdamOptimizer(),
                           metrics={"acc": acc})
```

Keras API:
```python
model = keras.Sequential(
    [keras.layers.Conv2D(20, kernel_size=(5, 5), strides=(1, 1), activation='tanh',
                         input_shape=(28, 28, 1), padding='valid'),
     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
     keras.layers.Conv2D(50, kernel_size=(5, 5), strides=(1, 1), activation='tanh',
                         padding='valid'),
     keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
     keras.layers.Flatten(),
     keras.layers.Dense(500, activation='tanh'),
     keras.layers.Dense(10, activation='softmax'),
     ]
)

model.compile(optimizer=keras.optimizers.RMSprop(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

est = Estimator.from_keras(keras_model=model)
```

Then users can perform distributed model training and inference as follows:

Distributed Training:
```python
mnist_train = tfds.load(name="mnist", split="train", data_dir=dataset_dir)
mnist_train = mnist_train.map(preprocess)
est.fit(data=mnist_train,
        batch_size=320,
        epochs=max_epoch)
```
The `data` argument in `fit` method can be a spark DataFrame, a XShards or a `tf.data.Dataset`. See the *data-parallel processing pipeline* [page](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) for more details.

Inference:
```python
df = spark.read.parquet("data.parquet")
predictions = est.predict(data=df,
                          feature_cols=['image'])
```
The `data` argument in `predict` method can be a spark DataFrame or a XShards. See the *data-parallel processing pipeline* [page](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) for more details.

The complete example can be found in [here](https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/orca/learn/tf).

#### **2.2 TensorFlow 2.x and Keras 2.4+**

Users of TensorFlow 2.x can create a TensorFlow `Estimator` from a keras model. The keras model should be created, compiled and returned in a python function. For example:

```python
def model_creator(config):
    import tensorflow as tf
    model = tf.keras.Sequential(
        [tf.keras.layers.Conv2D(20, kernel_size=(5, 5), strides=(1, 1), activation='tanh',
                                input_shape=(28, 28, 1), padding='valid'),
         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
         tf.keras.layers.Conv2D(50, kernel_size=(5, 5), strides=(1, 1), activation='tanh',
                                padding='valid'),
         tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid'),
         tf.keras.layers.Flatten(),
         tf.keras.layers.Dense(500, activation='tanh'),
         tf.keras.layers.Dense(10, activation='softmax'),
         ]
    )

    model.compile(optimizer=tf.keras.optimizers.RMSprop(),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

est = Estimator.from_keras(model_creator=model_creator)
```

Then users can perform distributed model training and inference as follows:

Distributed Training:
```python
import tensorflow as tf
import tensorflow_datasets as tfds
def train_data_creator(config):
    dataset = tfds.load(name="mnist", split="train", data_dir=dataset_dir)
    dataset = dataset.map(preprocess)
    dataset = dataset.shuffle(1000)
    dataset = dataset.batch(config["batch_size"])
    return dataset

stats = est.fit(data=train_data_creator,
                epochs=max_epoch,
                steps_per_epoch=total_size // batch_size)
```
The `data` argument in `fit` method can be a spark DataFrame, a XShards or a function that returns a `tf.data.Dataset`. See the *data-parallel processing pipeline* [page](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) for more details.

Inference:
```python
df = spark.read.parquet("data.parquet")
predictions = est.predict(data=df,
                          feature_cols=['image'])
```
The `data` argument in `predict` method can be a spark DataFrame or a XShards. See the *data-parallel processing pipeline* [page](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/data-parallel-processing.html) for more details.

The complete example can be found in [here](https://github.com/intel-analytics/analytics-zoo/tree/master/pyzoo/zoo/examples/orca/learn/tf2).

***For more details, view the distributed TensorFlow training/inference [page](). <TODO: link to be added>***

### **3. PyTorch Estimator**

**Using *BigDL* backend**

The user may create a PyTorch `Estimator` using the *BigDL* backend (currently default for PyTorch) as follows: <TODO: add a simple example>

Then the user can perform distributed model training and inference as follows: <TODO: add a simple example>

The input to `fit` and `predict` methods can be `torch.utils.data.DataLoader`, *XShards*, or a *Data Creator Function* (which returns `torch.utils.data.DataLoader`). See the *data-parallel processing pipeline* [page]() for more details. <TODO: we need to add Spark Dataframe support too>

View the related [Python API doc]() for more details.

**Using `torch.distributed` or *Horovod* backend**

<TODO: add description for `torch.distributed` or *Horovod* support>

***For more details, view the distributed PyTorch training/inference [page]().*** 

### **4. MXNet Estimator**

The user may create a MXNet `Estimator` as follows:
```python
import mxnet as mx
from mxnet import gluon
from mxnet.gluon import nn
from zoo.orca.learn.mxnet import Estimator, create_config

def get_model(config):
    class SimpleModel(gluon.Block):
        def __init__(self, **kwargs):
            super(SimpleModel, self).__init__(**kwargs)
            self.fc1 = nn.Dense(20)
            self.fc2 = nn.Dense(10)

        def forward(self, x):
            x = self.fc1(x)
            x = self.fc2(x)
            return x

    net = SimpleModel()
    net.initialize(mx.init.Xavier(magnitude=2.24), ctx=[mx.cpu()])
    return net

def get_loss(config):
    return gluon.loss.SoftmaxCrossEntropyLoss()

config = create_config(log_interval=2, optimizer="adam",
                       optimizer_params={'learning_rate': 0.02})
est = Estimator.from_mxnet(config=config,
                           model_creator=get_model,
                           loss_creator=get_loss,
                           num_workers=2)
```

Then the user can perform distributed model training as follows:
```python
import numpy as np

def get_train_data_iter(config, kv):
    train_data = np.random.rand(200, 30)
    train_label = np.random.randint(0, 10, (200,))
    train = mx.io.NDArrayIter(train_data, train_label,
                              batch_size=config["batch_size"], shuffle=True)
    return train

est.fit(get_train_data_iter, epochs=2)
```

The input to `fit` methods can be *XShards*, or a *Data Creator Function* (which takes config and kv as arguments and returns an `MXNet DataIter/DataLoader` for training). See the *data-parallel processing pipeline* [page](./data-parallel-processing.html) for more details.

View the related [Python API doc]() for more details.

### **5. BigDL Estimator**

The user may create a BigDL `Estimator` as follows:
```python
from bigdl.nn.criterion import *
from bigdl.nn.layer import *
from bigdl.optim.optimizer import *
from zoo.orca.learn.bigdl import Estimator

linear_model = Sequential().add(Linear(2, 2))
mse_criterion = MSECriterion()
est = Estimator.from_bigdl(model=linear_model, loss=mse_criterion, optimizer=Adam())
```

Then the user can perform distributed model training and inference as follows:
```python
# read spark Dataframe
df = spark.read.parquet("data.parquet")

# distributed model training
est.fit(df, 1, batch_size=4)

#distributed model inference
result_df = est.predict(df)
```

The input to `fit` and `predict` methods can be *XShards*, or a *Spark Dataframe*. See the *data-parallel processing pipeline* [page](./data-parallel-processing.html) for more details.

View the related [Python API doc]() for more details.

### **6. OpenVINO Estimator**

The user may create a OpenVINO `Estimator` as follows:
```python
from zoo.orca.learn.openvino import Estimator

model_path = "The/file_path/to/the/OpenVINO_IR_xml_file"
est = Estimator.from_openvino(model_path=model_path)
```

Then the user can perform distributed model inference as follows:
```python
# ndarray
input_data = np.random.random([20, 4, 3, 224, 224])
result = est.predict(input_data)

# xshards
shards = XShards.partition({"x": input_data})
result_shards = est.predict(shards)
```

The input to `predict` methods can be *XShards*, or a *numpy array*. See the *data-parallel processing pipeline* [page](./data-parallel-processing.html) for more details.

View the related [Python API doc]() for more details.
