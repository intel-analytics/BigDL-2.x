{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF Recommender with Explict Feedback using Orca data-preprocessing and TF Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demostrate how to leverage Orca data preprocessing and TF Estimator to scale python-based preprocessing and Tensorflow Training into Big Data cluster.\n",
    "\n",
    "In this example, we build a neural network recommendation system, Neural Collaborative Filtering(NCF) with explict feedback. \n",
    "\n",
    "We use Orca preprocessing to do the Pandas Preprocessing in parallel and leverage Orca Tensorflow Estimator to train a Tensorflow graph model in the same cluster. \n",
    "\n",
    "For Orca Data Preprocessing, please refer [Orca Data](https://analytics-zoo.github.io/master/#Orca/data/).\n",
    "\n",
    "For Orca Tensorflow Estimator, please refer [Orca TF Estimator](https://analytics-zoo.github.io/master/#Orca/orca-tf-estimator/).\n",
    "\n",
    "The system ([Recommendation systems: Principles, methods and evaluation](http://www.sciencedirect.com/science/article/pii/S1110866515000341)) normally prompts the user through the system interface to provide ratings for items in order to construct and improve the model. The accuracy of recommendation depends on the quantity of ratings provided by the user.  \n",
    "\n",
    "NCF([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)) leverages a multi-layer perceptrons to learn the userâ€“item interaction function, at the mean time, NCF can express and generalize matrix factorization under its framework. includeMF(Boolean) is provided for users to build a NCF with or without matrix factorization. \n",
    "\n",
    "Data: \n",
    "* The dataset we used is movielens-1M ([link](https://grouplens.org/datasets/movielens/1m/)), which contains 1 million ratings from 6000 users on 4000 movies.  There're 5 levels of rating. We will try classify each (user,movie) pair into 5 classes and evaluate the effect of algortithms using Mean Absolute Error.  \n",
    "  \n",
    "References: \n",
    "* A Tensorflow implementation of NCF model Recommendation([ali-ncf](https://github.com/alibaba/ai-matrix/tree/master/macro_benchmark/NCF)).\n",
    "* Nerual Collaborative filtering ([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/zoo_optimizer.py:73: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bigdl.dataset import base\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import zoo.orca.learn.tf.estimator\n",
    "from zoo.orca.data import SharedValue\n",
    "import zoo.orca.data.pandas\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Download movielens 1M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE_URL = 'http://files.grouplens.org/datasets/movielens/'\n",
    "WHOLE_DATA = 'ml-1m.zip'\n",
    "data_dir='/tmp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_file = base.maybe_download(WHOLE_DATA, data_dir, SOURCE_URL + WHOLE_DATA)\n",
    "zip_ref = zipfile.ZipFile(local_file, 'r')\n",
    "extracted_to = os.path.join(data_dir, \"ml-1m\")\n",
    "if not os.path.exists(extracted_to):\n",
    "    print(\"Extracting %s to %s\" % (local_file, data_dir))\n",
    "    zip_ref.extractall(data_dir)\n",
    "    zip_ref.close()\n",
    "rating_files = os.path.join(extracted_to, \"ratings.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Replace \"::\" to \":\" in ratings.dat and save to ratings_new.dat for spark 2.4 read csv support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rating_files = os.path.join(extracted_to, \"ratings_new.dat\")\n",
    "if not os.path.exists(new_rating_files):\n",
    "    fin = open(rating_files, \"rt\")\n",
    "    # output file to write the result to\n",
    "    fout = open(new_rating_files, \"wt\")\n",
    "    # for each line in the input file\n",
    "    for line in fin:\n",
    "        # read replace the string and write to output file\n",
    "        fout.write(line.replace('::', ':'))\n",
    "    # close input and output files\n",
    "    fin.close()\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file to XShards of Pandas DataFrame in parallel on Spark using Orca Data Preprocessing API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = ['user', 'item', 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = zoo.orca.data.pandas.read_csv(new_rating_files, sep=':', header=None, names=COLUMN_NAMES,\n",
    "                                              usecols=[0, 1, 2], dtype={0: np.int32, 1: np.int32, 2: np.int32}) \\\n",
    "        .partition_by('user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create user_id -> user_index, item_id -> item_index map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user size 6040\n",
      "item size 3706\n"
     ]
    }
   ],
   "source": [
    "user_set = set(full_data['user'].unique())\n",
    "item_set = set(full_data['item'].unique())\n",
    "\n",
    "user_size = len(user_set)\n",
    "item_size = len(item_set)\n",
    "print('user size %d' % user_size)\n",
    "print('item size %d' % item_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_index(s):\n",
    "    \"\"\" for reindexing the item set. \"\"\"\n",
    "    i = 0\n",
    "    s_map = {}\n",
    "    for key in s:\n",
    "        s_map[key] = i\n",
    "        i += 1\n",
    "\n",
    "    return s_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map = re_index(user_set)\n",
    "item_map = re_index(item_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change user id to user index, item id to item index with Pandas operation in parallel using Orca XShards transform_shard() API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_user_item(df, item_map, user_map):\n",
    "    user_list = []\n",
    "    item_list = []\n",
    "    item_map = item_map.value\n",
    "    user_map = user_map.value\n",
    "    for i in range(len(df)):\n",
    "        user_list.append(user_map[df['user'][i]])\n",
    "        item_list.append(item_map[df['item'][i]])\n",
    "    df['user'] = user_list\n",
    "    df['item'] = item_list\n",
    "    return df\n",
    "\n",
    "user_map_shared_value = SharedValue(user_map)\n",
    "item_map_shared_value = SharedValue(item_map)\n",
    "\n",
    "full_data = full_data.transform_shard(set_user_item, item_map_shared_value, user_map_shared_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update label starting from 0 using XShards.transform_shard() API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_label(df):\n",
    "    df['label'] = df['label'] - 1\n",
    "    return df\n",
    "\n",
    "full_data = full_data.transform_shard(update_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split to train/test dataset using scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train/test dataset\n",
    "def split_train_test(data):\n",
    "    # splitting the full set into train and test sets.\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=100)\n",
    "    return (train, test)\n",
    "\n",
    "train_data, test_data = full_data.transform_shard(split_train_test).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change to XShards of dictionary of x, y before train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_train_val_shard(df):\n",
    "    result = {\n",
    "        \"x\": (df['user'].to_numpy(), df['item'].to_numpy()),\n",
    "        \"y\": df['label'].to_numpy()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "train_data = train_data.transform_shard(to_train_val_shard)\n",
    "test_data = test_data.transform_shard(to_train_val_shard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(object):\n",
    "    def __init__(self, embed_size, user_size, item_size):\n",
    "        self.user = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "        self.item = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "        self.label = tf.placeholder(dtype=tf.int32, shape=(None,))\n",
    "\n",
    "        with tf.name_scope(\"GMF\"):\n",
    "            user_embed_GMF = tf.contrib.layers.embed_sequence(self.user,\n",
    "                                                              vocab_size=user_size,\n",
    "                                                              embed_dim=embed_size,\n",
    "                                                              unique=False\n",
    "                                                              )\n",
    "            item_embed_GMF = tf.contrib.layers.embed_sequence(self.item,\n",
    "                                                              vocab_size=item_size,\n",
    "                                                              embed_dim=embed_size,\n",
    "                                                              unique=False\n",
    "                                                              )\n",
    "            GMF = tf.multiply(user_embed_GMF, item_embed_GMF, name='GMF')\n",
    "\n",
    "        # MLP part starts\n",
    "        with tf.name_scope(\"MLP\"):\n",
    "            user_embed_MLP = tf.contrib.layers.embed_sequence(self.user,\n",
    "                                                              vocab_size=user_size,\n",
    "                                                              embed_dim=embed_size,\n",
    "                                                              unique=False,\n",
    "                                                              )\n",
    "\n",
    "            item_embed_MLP = tf.contrib.layers.embed_sequence(self.item,\n",
    "                                                              vocab_size=item_size,\n",
    "                                                              embed_dim=embed_size,\n",
    "                                                              unique=False\n",
    "                                                              )\n",
    "            interaction = tf.concat([user_embed_MLP, item_embed_MLP],\n",
    "                                    axis=-1, name='interaction')\n",
    "\n",
    "            layer1_MLP = tf.layers.dense(inputs=interaction,\n",
    "                                         units=embed_size * 2,\n",
    "                                         name='layer1_MLP')\n",
    "            layer1_MLP = tf.layers.dropout(layer1_MLP, rate=0.2)\n",
    "\n",
    "            layer2_MLP = tf.layers.dense(inputs=layer1_MLP,\n",
    "                                         units=embed_size,\n",
    "                                         name='layer2_MLP')\n",
    "            layer2_MLP = tf.layers.dropout(layer2_MLP, rate=0.2)\n",
    "\n",
    "            layer3_MLP = tf.layers.dense(inputs=layer2_MLP,\n",
    "                                         units=embed_size // 2,\n",
    "                                         name='layer3_MLP')\n",
    "            layer3_MLP = tf.layers.dropout(layer3_MLP, rate=0.2)\n",
    "\n",
    "        # Concate the two parts together\n",
    "        with tf.name_scope(\"concatenation\"):\n",
    "            concatenation = tf.concat([GMF, layer3_MLP], axis=-1,\n",
    "                                      name='concatenation')\n",
    "            self.logits = tf.layers.dense(inputs=concatenation,\n",
    "                                          units=5,\n",
    "                                          name='predict')\n",
    "            \n",
    "            self.logits_softmax = tf.nn.softmax(self.logits)\n",
    "\n",
    "            self.class_number = tf.argmax(self.logits_softmax, 1)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.label, logits=self.logits, name='loss'))\n",
    "\n",
    "        with tf.name_scope(\"optimzation\"):\n",
    "            self.optim = tf.train.AdamOptimizer(1e-3, name='Adam')\n",
    "            self.optimzer = self.optim.minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-14-ac038f46aa4e>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /home/jwang/miniconda3/envs/tf36/lib/python3.6/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-14-ac038f46aa4e>:39: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 16\n",
    "model = NCF(embedding_size, user_size, item_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Orca TF Estimator for training/validation/prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/util/tf.py:31: The name tf.unsorted_segment_sum is deprecated. Please use tf.math.unsorted_segment_sum instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/orca/learn/tf/estimator.py:235: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/orca/learn/tf/estimator.py:236: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./\"\n",
    "\n",
    "estimator = zoo.orca.learn.tf.estimator.Estimator.from_graph(\n",
    "            inputs=[model.user, model.item],\n",
    "            outputs=[model.class_number],\n",
    "            labels=[model.label],\n",
    "            loss=model.loss,\n",
    "            optimizer=model.optim,\n",
    "            model_dir=model_dir,\n",
    "            metrics={\"loss\": model.loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TF model in parallel on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_dataset.py:209: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_dataset.py:210: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
      "\n",
      "creating: createFakeOptimMethod\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:295: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:159: The name tf.is_numeric_tensor is deprecated. Please use tf.debugging.is_numeric_tensor instead.\n",
      "\n",
      "creating: createStatelessMetric\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:187: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:202: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:243: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:276: The name tf.tables_initializer is deprecated. Please use tf.compat.v1.tables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/spark-071ba804-7405-47bc-84a6-adcfd2062040/userFiles-2165c963-919f-4942-a319-a804f2ccce5e/analytics-zoo-bigdl_0.11.1-spark_2.4.3-0.9.0-SNAPSHOT-python-api.zip/zoo/tfpark/tf_optimizer.py:285: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "creating: createTFTrainingHelper\n",
      "creating: createIdentityCriterion\n",
      "creating: createTFParkSampleToMiniBatch\n",
      "creating: createTFParkSampleToMiniBatch\n",
      "creating: createEstimator\n",
      "creating: createMaxEpoch\n",
      "creating: createEveryEpoch\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmp2txi_b2r/model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<zoo.orca.learn.tf.estimator.TFOptimizerWrapper at 0x7f25dc1f5390>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1280\n",
    "epochs = 10\n",
    "\n",
    "estimator.fit(data=train_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=test_data\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save TF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model in Tensorflow checkpoint for later evaluation and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing list_input_0:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'TFNdarrayDataset' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing list_input_1:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'TFNdarrayDataset' object has no attribute 'name'\n",
      "WARNING:tensorflow:Issue encountered when serializing list_input_0_1:0.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'TFNdarrayDataset' object has no attribute 'name'\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join(model_dir, \"NCF.ckpt\")\n",
    "estimator.save_tf_checkpoint(checkpoint_path)\n",
    "estimator.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restore saved Tensorflow checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./NCF.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "model = NCF(embedding_size, user_size, item_size)\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "checkpoint_path = os.path.join(model_dir, \"NCF.ckpt\")\n",
    "saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new TF Estimator to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = zoo.orca.learn.tf.estimator.Estimator.from_graph(\n",
    "        inputs=[model.user, model.item],\n",
    "        outputs=[model.class_number],\n",
    "        labels=[model.label],\n",
    "        sess=sess,\n",
    "        model_dir=model_dir\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare predict XShards. The predict XShards should only contain 'x' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_predict(data):\n",
    "    del data['y']\n",
    "    return data\n",
    "\n",
    "predict_data = test_data.transform_shard(to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict data XShards. The prediction result is also an XShards, but only contains 'prediction' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /tmp/tmpaxod7ns5/saved_model.pb\n",
      "[2. 3. 2. ... 0. 2. 3.]\n"
     ]
    }
   ],
   "source": [
    "predict_result = estimator.predict(predict_data)\n",
    "predictions = predict_result.collect()\n",
    "print(predictions[0]['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
