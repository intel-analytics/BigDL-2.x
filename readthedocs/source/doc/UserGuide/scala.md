# Scala User Guide

---

### **1. Install**

#### **1.1 Official Release** 

Currently, Analytics Zoo releases are hosted on maven central; here's an example to add the Analytics Zoo dependency to your own project:
```xml
<dependency>
    <groupId>com.intel.analytics.zoo</groupId>
    <artifactId>analytics-zoo-bigdl_0.12.1-spark_2.4.3</artifactId>
    <version>0.9.0</version>
</dependency>
```
You can find the other SPARK version [here](https://search.maven.org/search?q=analytics-zoo-bigdl), such as `spark_2.1.1, spark_2.2.1, spark_2.3.1, spark_3.0.0`.   


SBT developers can use
```sbt
libraryDependencies += "com.intel.analytics.zoo" % "analytics-zoo-bigdl_0.12.1-spark_2.4.3" % "0.9.0"
```

#### **1.2 Nightly Build**

Currently, Analytics Zoo development version is hosted on [SonaType](https://oss.sonatype.org/content/groups/public/com/intel/analytics/zoo/).

To link your application with the latest Analytics Zoo development version, you should add some dependencies like [official releases](#11-official-release), but set `0.9.0` to snapshot version, and add below repository to your pom.xml.

```xml
<repository>
    <id>sonatype</id>
    <name>sonatype repository</name>
    <url>https://oss.sonatype.org/content/groups/public/</url>
    <releases>
        <enabled>true</enabled>
    </releases>
    <snapshots>
        <enabled>true</enabled>
    </snapshots>
</repository>
```

SBT developers can use
```sbt
resolvers += "ossrh repository" at "https://oss.sonatype.org/content/repositories/snapshots/"
```

#### **1.3 Download Pre-Built Package**

You can download the Analytics Zoo release and nightly build from the [Release Page](../release.md)

### **2. Run**

Before running Analytics Zoo, Environment Variables **ANALYTICS_ZOO_HOME** and **SPARK_HOME** should be set:

* If you download Analytics Zoo from the [Release Page](../release-download.md)
```bash
export SPARK_HOME=folder path where you extract the spark package
export ANALYTICS_ZOO_HOME=folder path where you extract the analytics zoo package
```

* If you build Analytics Zoo by yourself
```bash
export SPARK_HOME=folder path where you extract the spark package
export ANALYTICS_ZOO_HOME=the dist folder generated by the build process, which is under the top level of the source folder
```

---
#### **2.1 Use Interactive Spark Shell**
You can try Analytics Zoo easily using the Spark interactive shell. Run below command to start spark shell with Analytics Zoo support:
```bash
${ANALYTICS_ZOO_HOME}/bin/spark-shell-with-zoo.sh --master local[1]
```
You will see a welcome message looking like below:
```
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_112)
Type in expressions to have them evaluated.
Type :help for more information.
```

Now you'll be able to play with Analytics Zoo API's.
For instance, to train a linear regression model, you may try below code:
```scala
scala> import com.intel.analytics.zoo.common.NNContext
import com.intel.analytics.zoo.common.NNContext

scala> import com.intel.analytics.bigdl.nn._
import com.intel.analytics.bigdl.nn._

scala> import com.intel.analytics.zoo.pipeline.nnframes.NNEstimator
import com.intel.analytics.zoo.pipeline.nnframes.NNEstimator

scala> import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat
import com.intel.analytics.bigdl.tensor.TensorNumericMath.TensorNumeric.NumericFloat

scala> import org.apache.spark.sql.{DataFrame, SQLContext}
import org.apache.spark.sql.{DataFrame, SQLContext}

scala> val sc = NNContext.initNNContext("Run Example")
2021-01-26 10:19:52 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
2021-01-26 10:19:53 WARN  SparkContext:66 - Using an existing SparkContext; some configuration may not take effect.
sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@487f025

scala> val sqlContext = SQLContext.getOrCreate(sc)
warning: there was one deprecation warning; re-run with -deprecation for details
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@39d62e47

scala> val model = Sequential().add(Linear(2, 2))
model: com.intel.analytics.bigdl.nn.Sequential[Float] =
Sequential[6f42ba4c]{
  [input -> (1) -> output]
  (1): Linear[1178e3b8](2 -> 2)
}

scala> val criterion = MSECriterion()
criterion: com.intel.analytics.bigdl.nn.MSECriterion[Float] = com.intel.analytics.bigdl.nn.MSECriterion$mcF$sp@0

scala> val estimator = NNEstimator(model, criterion).setLearningRate(0.2).setMaxEpoch(40).setBatchSize(2)
warning: there was one deprecation warning; re-run with -deprecation for details
estimator: com.intel.analytics.zoo.pipeline.nnframes.NNEstimator[Float] = nnestimator_628627bc59c2

scala> val data = sc.parallelize(Seq(
     |   (Array(2.0, 1.0), Array(1.0, 2.0)),
     |   (Array(1.0, 2.0), Array(2.0, 1.0)),
     |   (Array(2.0, 1.0), Array(1.0, 2.0)),
     |   (Array(1.0, 2.0), Array(2.0, 1.0))))
data: org.apache.spark.rdd.RDD[(Array[Double], Array[Double])] = ParallelCollectionRDD[0] at parallelize at <console>:32

scala> val df = sqlContext.createDataFrame(data).toDF("features", "label")
df: org.apache.spark.sql.DataFrame = [features: array<double>, label: array<double>]

scala> val nnModel = estimator.fit(df)
2021-01-26 10:19:56 WARN  BlockManager:66 - Asked to remove block test_0weights0, which does not exist
2021-01-26 10:19:56 WARN  BlockManager:66 - Asked to remove block test_0gradients0, which does not exist
nnModel: com.intel.analytics.zoo.pipeline.nnframes.NNModel[Float] = DLModel

scala> nnModel.transform(df).show(false)
+----------+----------+----------------------+
|features  |label     |prediction            |
+----------+----------+----------------------+
|[2.0, 1.0]|[1.0, 2.0]|[1.0099465, 2.0036266]|
|[1.0, 2.0]|[2.0, 1.0]|[2.013829, 1.0022666] |
|[2.0, 1.0]|[1.0, 2.0]|[1.0099465, 2.0036266]|
|[1.0, 2.0]|[2.0, 1.0]|[2.013829, 1.0022666] |
+----------+----------+----------------------+
```

---

#### **2.2 Run Analytics Zoo's example**
You can run a analytics zoo program, e.g., the [Wide&Deep Recommendation](https://github.com/intel-analytics/analytics-zoo/tree/master/zoo/src/main/scala/com/intel/analytics/zoo/examples/recommendation), as a standard Spark program (running in either local mode or cluster mode) as follows:

1. Download Census Income Data Set from [here](https://archive.ics.uci.edu/ml/datasets/Census+Income).
2. Run the following command:
```bash
# Spark local mode
${ANALYTICS_ZOO_HOME}/bin/spark-submit-scala-with-zoo.sh \ 
  --master local[core_number] \
  --class com.intel.analytics.zoo.examples.recommendation.WideAndDeepExample \
  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \
  --inputDir ./data/census \
  --batchSize 320 \
  --maxEpoch 20 \
  --dataset census

# Spark standalone mode
${ANALYTICS_ZOO_HOME}/bin/spark-submit-scala-with-zoo.sh \
  --master spark://... \
  --executor-cores cores_per_executor \
  --total-executor-cores total_cores_for_the_job \
  --class com.intel.analytics.zoo.examples.recommendation.WideAndDeepExample \
  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \
  --inputDir ./data/census \
  --batchSize 320 \
  --maxEpoch 20 \
  --dataset census

# Spark yarn client mode
${ANALYTICS_ZOO_HOME}/bin/spark-submit-scala-with-zoo.sh \
  --master yarn \
  --deploy-mode client \
  --executor-cores cores_per_executor \
  --num-executors executors_number \
  --class com.intel.analytics.zoo.examples.recommendation.WideAndDeepExample \
  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \
  --inputDir ./data/census \
  --batchSize 320 \
  --maxEpoch 20 \
  --dataset census

# Spark yarn cluster mode
${ANALYTICS_ZOO_HOME}/bin/spark-submit-scala-with-zoo.sh \
  --master yarn \
  --deploy-mode cluster \
  --executor-cores cores_per_executor \
  --num-executors executors_number \
  --class com.intel.analytics.zoo.examples.recommendation.WideAndDeepExample \
  dist/lib/analytics-zoo-VERSION-jar-with-dependencies.jar \
  --inputDir ./data/census \
  --batchSize 320 \
  --maxEpoch 20 \
  --dataset census
```

### **2.3 Run user application**
Here is a [simple MLP example](https://github.com/qiuxin2012/SimpleMlp) to show you how to intergate analytics zoo to your project with maven or sbt, and how to build and run your own deep learning application in IDEA and spark-submit.
