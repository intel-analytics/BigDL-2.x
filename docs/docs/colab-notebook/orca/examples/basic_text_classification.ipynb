{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgwhat/analytics-zoo/blob/colab-examples/docs/docs/colab-notebook/orca/examples/basic_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnqB6ax-EPoB"
      },
      "outputs": [],
      "source": [
        "#\n",
        "# Copyright 2018 Analytics Zoo Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "#\n",
        "# Copyright 2018 The TensorFlow Authors.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "# MIT License\n",
        "#\n",
        "# Copyright (c) 2017 FranÃ§ois Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE.\n",
        "#\n",
        "# This example classifies movie reviews as positive or negative using the text of the review,\n",
        "# and is adapted from\n",
        "# https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/keras/basic_text_classification.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ResW0PSY4xWC"
      },
      "source": [
        "## **Environment Preparation**\n",
        "\n",
        "**Install Java 8**\n",
        "\n",
        "Run the cell on the **Google Colab** to install jdk 1.8.\n",
        "\n",
        "**Note:** if you run this notebook on your computer, root permission is required when running the cell to install Java 8. (You may ignore this cell if Java 8 has already been set up in your computer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0p4mAbe0iH4"
      },
      "outputs": [],
      "source": [
        "# Install jdk8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "# Set environment variable JAVA_HOME.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ece-xZYi8OIz"
      },
      "source": [
        "**Install Analytics Zoo**\n",
        "\n",
        "[Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) is needed to prepare the Python environment for running this example. \n",
        "\n",
        "**Note**: The following code cell is specific for setting up conda environment on Colab; for general conda installation, please refer to the [install guide](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69suYs9K0kiu"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Get current python version\n",
        "version_info = sys.version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JczTAk5E0nW2"
      },
      "outputs": [],
      "source": [
        "# Install Miniconda\n",
        "!wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Update Conda\n",
        "!conda install --channel defaults conda python=$python_version --yes\n",
        "!conda update --channel defaults --all --yes\n",
        "\n",
        "# Append to the sys.path\n",
        "_ = (sys.path\n",
        "        .append(f\"/usr/local/lib/python{version_info.major}.{version_info.minor}/site-packages\"))\n",
        "\n",
        "os.environ['PYTHONHOME']=\"/usr/local\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_OS4HKJMNpv"
      },
      "source": [
        "You can install the latest pre-release version using `pip install --pre --upgrade analytics-zoo`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qECDX3mlQV9I"
      },
      "outputs": [],
      "source": [
        "# Install latest pre-release version of Analytics Zoo \n",
        "# Installing Analytics Zoo from pip will automatically install pyspark, bigdl, and their dependencies.\n",
        "!pip install --pre --upgrade analytics-zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EeQMD1oswNNC"
      },
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "# The tutorial below only supports TensorFlow 1.15\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXA_MBBbgf-l"
      },
      "outputs": [],
      "source": [
        "# Switch tensorflow to 1.x version\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mo8uojl9rQN"
      },
      "source": [
        "## **Distributed Kreas using Orca APIs**\n",
        "\n",
        "In this guide we will describe how to scale out Keras programs using Orca in 4 simple steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-mCzxMaN-Fc"
      },
      "outputs": [],
      "source": [
        "#import necessary libraries and modules\n",
        "#from __future__ import print_function\n",
        "from zoo.orca import init_orca_context, stop_orca_context\n",
        "from zoo.orca import OrcaContext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrFNoK26_Dzp"
      },
      "source": [
        "### **Step 1: Init Orca Context**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOmeBIPlxk-p"
      },
      "outputs": [],
      "source": [
        "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
        "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
        "\n",
        "cluster_mode = \"local\"\n",
        "\n",
        "if cluster_mode == \"local\":\n",
        "    init_orca_context(cores=1, memory=\"2g\") # run in local mode\n",
        "elif cluster_mode == \"k8s\":\n",
        "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
        "elif cluster_mode == \"yarn\":\n",
        "    init_orca_context(\n",
        "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
        "        driver_memory=\"10g\", driver_cores=1,\n",
        "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
        "              \"spark.task.maxFailures\": \"1\",\n",
        "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"}) # run on Hadoop YARN cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUOk_SJRSfbD"
      },
      "source": [
        "This is the only place where you need to specify local or distributed mode. View [Orca Context](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca-context.html) for more details.\n",
        "\n",
        "**Note**: You should export HADOOP_CONF_DIR=/path/to/hadoop/conf/dir when you run on Hadoop YARN cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6q-5q_H_ZFc"
      },
      "source": [
        "### **Step 2: Define the Model**\n",
        "You may define your model, loss and optimizer in the same way as in any standard (single node) Keras program."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-sHSS4yf5Kp"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# input shape is the vocabulary count used for the movie reviews (10,000 words)\n",
        "vocab_size = 10000\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Embedding(vocab_size, 16))\n",
        "model.add(keras.layers.GlobalAveragePooling1D())\n",
        "model.add(keras.layers.Dense(16, activation=tf.nn.relu))\n",
        "model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "007mkKyaCu1I"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR_6N61wADCI"
      },
      "source": [
        "### **Step 3: Define Train Dataset**\n",
        "\n",
        "**Prepare Dataset**\n",
        "\n",
        "The IMDB dataset comes packaged with TensorFlow. It has already been preprocessed such that the reviews (sequences of words) have been converted to sequences of integers, where each integer represents a specific word in a dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BDzZPwQA27B"
      },
      "outputs": [],
      "source": [
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=vocab_size)\n",
        "print(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n",
        "\n",
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "# The first indices are reserved\n",
        "word_index = {k: (v + 3) for k, v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_56v6UpA27E"
      },
      "outputs": [],
      "source": [
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(train_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOawME9fMhjy"
      },
      "source": [
        "Transform a list (of length num_samples) of sequences (lists of integers) into a 2D Numpy array of shape (num_samples, num_timesteps). \n",
        "num_timesteps is either the maxlen argument if provided, or the length of the longest sequence in the list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSbWCUNAhF4u"
      },
      "outputs": [],
      "source": [
        "train_data = keras.preprocessing.sequence.pad_sequences(train_data, \n",
        "                                                        value=word_index[\"<PAD>\"], \n",
        "                                                        padding='post', \n",
        "                                                        maxlen=256)\n",
        "\n",
        "test_data = keras.preprocessing.sequence.pad_sequences(test_data, \n",
        "                                                       value=word_index[\"<PAD>\"], \n",
        "                                                       padding='post',  \n",
        "                                                       maxlen=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSl17hDqPMaa"
      },
      "source": [
        "Define the dataset using standard tf.data.Dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb1YS8zXh7bK"
      },
      "outputs": [],
      "source": [
        "x_val = train_data[:10000]\n",
        "partial_x_train = train_data[10000:]\n",
        "\n",
        "y_val = train_labels[:10000]\n",
        "partial_y_train = train_labels[10000:]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((partial_x_train, partial_y_train))\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKvWhWLrDYPp"
      },
      "source": [
        "### **Step 4: Fit with Orca Estimator**\n",
        "\n",
        "First, create an Estimator and set backend to `bigdl`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIty4PkUBVoA"
      },
      "outputs": [],
      "source": [
        "from zoo.orca.learn.tf.estimator import Estimator\n",
        "\n",
        "est = Estimator.from_keras(keras_model=model, backend='bigdl')\n",
        "\n",
        "tensorboard_dir = \"runs\"\n",
        "est.set_tensorboard(tensorboard_dir, \"bigdl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bNtls5WXnzU"
      },
      "source": [
        "Next, fit the Estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmyJERefC-u3"
      },
      "outputs": [],
      "source": [
        "est.fit(data=train_dataset,\n",
        "        batch_size=32,\n",
        "        epochs=1,\n",
        "        validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRdASNtRAbiN"
      },
      "source": [
        "Finally, evaluate using the Estimator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXxnyyeTC3jC"
      },
      "outputs": [],
      "source": [
        "results = est.evaluate(validation_dataset)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyRtP6lqieej"
      },
      "source": [
        "Now, the accuracy of this model has reached 85%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0Vm-JBHiBOa"
      },
      "outputs": [],
      "source": [
        "# Stop orca context when your program finishes\n",
        "stop_orca_context()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OBlnkPjM80S"
      },
      "source": [
        "### **Step 5: Visualization by Tensorboard**\n",
        "\n",
        "TensorBoard is a visualization toolkit for machine learning experimentation. TensorBoard allows tracking and visualizing metrics such as loss and accuracy, visualizing the model graph, viewing histograms, displaying images and much more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lNnhkX68cqW"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adXtrlX61ayB"
      },
      "source": [
        "A brief overview of the dashboards shown (tabs in top navigation bar):\n",
        "\n",
        "* The **SCALARS** dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
        "\n",
        "Start TensorBoard, specifying the root log directory you used above. \n",
        "Argument ``logdir`` points to directory where TensorBoard will look to find \n",
        "event files that it can display. TensorBoard will recursively walk \n",
        "the directory structure rooted at logdir, looking for .*tfevents.* files.\n",
        "This dashboard shows how the loss and accuracy change with every iteration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyI5lrXoMw9K"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir \"/content/runs/bigdl/train\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO4AQx7Kpo6y08KUGheJElc",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "basic_text_classification.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    },
    "kernelspec": {
      "display_name": "Python 3.9.4 64-bit",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}