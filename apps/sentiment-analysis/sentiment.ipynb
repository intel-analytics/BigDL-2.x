{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification on Large Movie Reviews\n",
    "\n",
    "[Sentiment Analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is understood as a classic natural language processing problem. In this example, a large moview review dataset was chosen from IMDB to do a sentiment classification task with some deep learning approaches. The labeled data set consists of 50,000 [IMDB](http://www.imdb.com/) movie reviews (good or bad), in which 25000 highly polar movie reviews for training, and 25,000 for testing. The dataset is originally collected by Stanford researchers and was used in a [2011 paper](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf), and the highest accuray of 88.33% was achieved without using the unbalanced data. This example illustrates some deep learning approaches to do the sentiment classification with [BigDL](https://github.com/intel-analytics/BigDL) python API.\n",
    "\n",
    "### Load the IMDB Dataset\n",
    "The IMDB dataset need to be loaded into BigDL, note that the dataset has been pre-processed, and each review was encoded as a sequence of integers. Each integer represents the index of the overall frequency of dataset, for instance, '5' means the 5-th most frequent words occured in the data. It is very convinient to filter the words by some conditions, for example, to filter only the top 5,000 most common word and/or eliminate the top 30 most common words. Let's define functions to load the pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "finished processing text\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dllib.feature.dataset import base\n",
    "import numpy as np\n",
    "\n",
    "def download_imdb(dest_dir):\n",
    "    \"\"\"Download pre-processed IMDB movie review data\n",
    "\n",
    "    :argument\n",
    "        dest_dir: destination directory to store the data\n",
    "\n",
    "    :return\n",
    "        The absolute path of the stored data\n",
    "    \"\"\"\n",
    "    file_name = \"imdb.npz\"\n",
    "    file_abs_path = base.maybe_download(file_name,\n",
    "                                        dest_dir,\n",
    "                                        'https://s3.amazonaws.com/text-datasets/imdb.npz')\n",
    "    return file_abs_path\n",
    "\n",
    "def load_imdb(dest_dir='/tmp/.bigdl/dataset'):\n",
    "    \"\"\"Load IMDB dataset.\n",
    "\n",
    "    :argument\n",
    "        dest_dir: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "\n",
    "    :return\n",
    "        the train, test separated IMDB dataset.\n",
    "    \"\"\"\n",
    "    path = download_imdb(dest_dir)\n",
    "    f = np.load(path, allow_pickle=True)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "print('Processing text dataset')\n",
    "(x_train, y_train), (x_test, y_test) = load_imdb()\n",
    "print('finished processing text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to set a proper max sequence length, we need to go througth the property of the data and see the length distribution of each sentence in the dataset. A box and whisker plot is shown below for reviewing the length distribution in words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Review length: \n",
      "Mean 233.76 words (172.911495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['cm']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline \n",
    "# Summarize review length\n",
    "from matplotlib import pyplot\n",
    "\n",
    "print(\"Review length: \")\n",
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "# Create a figure instance\n",
    "fig = pyplot.figure(1, figsize=(6, 6))\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking the box and whisker plot, the max length of a sample in words is 500, and the mean and median are below 250. According to the plot, we can probably cover the mass of the distribution with a clipped length of 400 to 500. Here we set the max sequence length of each sample as 500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding vocabulary sorted by frequency is also required, for further embedding the words with pre-trained vectors. The downloaded vocabulary is in {word: index}, where each word as a key and the index as a value. It needs to be transformed into {index: word} format.\n",
    "\n",
    "Let's define a function to obtain the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing vocabulary\n",
      "finished processing vocabulary\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def get_word_index(dest_dir='/tmp/.bigdl/dataset', ):\n",
    "    \"\"\"Retrieves the dictionary mapping word indices back to words.\n",
    "\n",
    "    :argument\n",
    "        path: where to cache the data (relative to `~/.bigdl/dataset`).\n",
    "\n",
    "    :return\n",
    "        The word index dictionary.\n",
    "    \"\"\"\n",
    "    file_name = \"imdb_word_index.json\"\n",
    "    path = base.maybe_download(file_name,\n",
    "                               dest_dir,\n",
    "                               source_url='https://s3.amazonaws.com/text-datasets/imdb_word_index.json')\n",
    "    f = open(path)\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    return data\n",
    "\n",
    "print('Processing vocabulary')\n",
    "word_idx = get_word_index()\n",
    "idx_word = {v:k for k,v in word_idx.items()}\n",
    "print('finished processing vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text pre-processing\n",
    "\n",
    "Before we train the network, some pre-processing steps need to be applied to the dataset. \n",
    "\n",
    "Next let's go through the mechanisms that used to be applied to the data.\n",
    "\n",
    "* We insert a `start_char` at the beginning of each sentence to mark the start point. We set it as `2` here, and each other word index will plus a constant `index_from` to differentiate some 'helper index' (eg. `start_char`, `oov_char`, etc.).\n",
    "\n",
    "* A `max_words` variable is defined as the maximum index number (the least frequent word) included in the sequence. If the word index number is larger than `max_words`, it will be replaced by a out-of-vocabulary number `oov_char`, which is `3` here.\n",
    "\n",
    "* Each word index sequence is restricted to the same length. We used left-padding here, which means the right (end) of the sequence will be keep as many as possible and drop the left (head) of the sequence if its length is more than pre-defined `sequence_len`, or padding the left (head) of the sequence with `padding_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start transformation\n",
      "finish transformation\n"
     ]
    }
   ],
   "source": [
    "def replace_oov(x, oov_char, max_words):\n",
    "    \"\"\"\n",
    "    Replace the words out of vocabulary with `oov_char`\n",
    "    :param x: a sequence\n",
    "    :param max_words: the max number of words to include\n",
    "    :param oov_char: words out of vocabulary because of exceeding the `max_words`\n",
    "        limit will be replaced by this character\n",
    "\n",
    "    :return: The replaced sequence\n",
    "    \"\"\"\n",
    "    return [oov_char if w >= max_words else w for w in x]\n",
    "\n",
    "def pad_sequence(x, fill_value, length):\n",
    "    \"\"\"\n",
    "    Pads each sequence to the same length\n",
    "    :param x: a sequence\n",
    "    :param fill_value: pad the sequence with this value\n",
    "    :param length: pad sequence to the length\n",
    "\n",
    "    :return: the padded sequence\n",
    "    \"\"\"\n",
    "    if len(x) >= length:\n",
    "        return x[(len(x) - length):]\n",
    "    else:\n",
    "        return [fill_value] * (length - len(x)) + x\n",
    "\n",
    "def to_sample(features, label):\n",
    "    \"\"\"\n",
    "    Wrap the `features` and `label` to a training sample object\n",
    "    :param features: features of a sample\n",
    "    :param label: label of a sample\n",
    "    \n",
    "    :return: a sample object including features and label\n",
    "    \"\"\"\n",
    "    return Sample.from_ndarray(np.array(features, dtype='float'), np.array(label))\n",
    "\n",
    "padding_value = 1\n",
    "start_char = 2\n",
    "oov_char = 3\n",
    "index_from = 3\n",
    "max_words = 5000\n",
    "sequence_len = 500\n",
    "\n",
    "print('start transformation')\n",
    "\n",
    "from bigdl.dllib.utils.nncontext import *\n",
    "sc = init_nncontext(\"Sentiment Analysis Example\")\n",
    "\n",
    "\n",
    "train_rdd = sc.parallelize(zip(x_train, y_train), 2) \\\n",
    "    .map(lambda record: ([start_char] + [w + index_from for w in record[0]], record[1])) \\\n",
    "    .map(lambda record: (replace_oov(record[0], oov_char, max_words), record[1])) \\\n",
    "    .map(lambda record: (pad_sequence(record[0], padding_value, sequence_len), record[1])) \\\n",
    "    .map(lambda record: to_sample(record[0], record[1]))\n",
    "test_rdd = sc.parallelize(zip(x_test, y_test), 2) \\\n",
    "    .map(lambda record: ([start_char] + [w + index_from for w in record[0]], record[1])) \\\n",
    "    .map(lambda record: (replace_oov(record[0], oov_char, max_words), record[1])) \\\n",
    "    .map(lambda record: (pad_sequence(record[0], padding_value, sequence_len), record[1])) \\\n",
    "    .map(lambda record: to_sample(record[0], record[1]))\n",
    "        \n",
    "print('finish transformation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Word Embedding\n",
    "\n",
    "[Word embedding](https://en.wikipedia.org/wiki/Word_embedding) is a recent breakthrough in natural language field. The key idea is to encode words and phrases into distributed representations in the format of word vectors, which means each word is represented as a vector. There are two widely used word vector training alogirhms, one is published by Google called [word to vector](https://arxiv.org/abs/1310.4546), the other is published by Standford called [Glove](https://nlp.stanford.edu/projects/glove/). In this example, pre-trained glove is loaded into a lookup table and will be fine-tuned during the training process. BigDL provides a method to download and load glove in `news20` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading glove\n",
      "finish loading glove\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dllib.feature.dataset import news20\n",
    "import itertools\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "print('loading glove')\n",
    "glove = news20.get_glove_w2v(source_dir='/tmp/.bigdl/dataset', dim=embedding_dim)\n",
    "print('finish loading glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each word whose index less than the `max_word` should try to match its embedding and store in an array.\n",
    "\n",
    "With regard to those words which can not be found in glove, we randomly sample it from a [-0.05, 0.05] uniform distribution.\n",
    "\n",
    "BigDL usually use a `LookupTable` layer to do word embedding, so the matrix will be loaded to the LookupTable by seting the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing glove\n",
      "finish processing glove\n"
     ]
    }
   ],
   "source": [
    "print('processing glove')\n",
    "w2v = [glove.get(idx_word.get(i - index_from), np.random.uniform(-0.05, 0.05, embedding_dim))\n",
    "        for i in range(1, max_words + 1)]\n",
    "w2v = np.array(list(itertools.chain(*np.array(w2v, dtype='float'))), dtype='float') \\\n",
    "        .reshape([max_words, embedding_dim])\n",
    "print('finish processing glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models\n",
    "\n",
    "Next, let's build some deep learning models for the sentiment classification. \n",
    "\n",
    "As an example, several deep learning models are illustrated for tutorial, comparison and demonstration.\n",
    "\n",
    "**LSTM**, **GRU**, **Bi-LSTM**, **CNN** and **CNN + LSTM** models are implemented as options. To decide which model to use, just assign model_type the corresponding string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.dllib.nn.layer import *\n",
    "\n",
    "p = 0.2\n",
    "\n",
    "def build_model(w2v):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding = LookupTable(max_words, embedding_dim)\n",
    "    embedding.set_weights([w2v])\n",
    "    model.add(embedding)\n",
    "    if model_type.lower() == \"gru\":\n",
    "        model.add(Recurrent()\n",
    "                .add(GRU(embedding_dim, 128, p))) \\\n",
    "            .add(Select(2, -1))\n",
    "    elif model_type.lower() == \"lstm\":\n",
    "        model.add(Recurrent()\n",
    "                  .add(LSTM(embedding_dim, 128, p)))\\\n",
    "            .add(Select(2, -1))\n",
    "    elif model_type.lower() == \"bi_lstm\":\n",
    "        model.add(BiRecurrent(CAddTable())\n",
    "                  .add(LSTM(embedding_dim, 128, p)))\\\n",
    "            .add(Select(2, -1))\n",
    "    elif model_type.lower() == \"cnn\":\n",
    "        model.add(Transpose([(2, 3)]))\\\n",
    "            .add(Dropout(p))\\\n",
    "            .add(Reshape([embedding_dim, 1, sequence_len]))\\\n",
    "            .add(SpatialConvolution(embedding_dim, 128, 5, 1))\\\n",
    "            .add(ReLU())\\\n",
    "            .add(SpatialMaxPooling(sequence_len - 5 + 1, 1, 1, 1))\\\n",
    "            .add(Reshape([128]))\n",
    "    elif model_type.lower() == \"cnn_lstm\":\n",
    "        model.add(Transpose([(2, 3)]))\\\n",
    "            .add(Dropout(p))\\\n",
    "            .add(Reshape([embedding_dim, 1, sequence_len])) \\\n",
    "            .add(SpatialConvolution(embedding_dim, 64, 5, 1)) \\\n",
    "            .add(ReLU()) \\\n",
    "            .add(SpatialMaxPooling(4, 1, 1, 1)) \\\n",
    "            .add(Squeeze(3)) \\\n",
    "            .add(Transpose([(2, 3)])) \\\n",
    "            .add(Recurrent()\n",
    "                 .add(LSTM(64, 128, p))) \\\n",
    "            .add(Select(2, -1))\n",
    "\n",
    "    model.add(Linear(128, 100))\\\n",
    "        .add(Dropout(0.2))\\\n",
    "        .add(ReLU())\\\n",
    "        .add(Linear(100, 1))\\\n",
    "        .add(Sigmoid())\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "`Optimizer` need to be created to optimise the model.\n",
    "\n",
    "Here we use the `CNN` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createLookupTable\n",
      "creating: createRecurrent\n",
      "creating: createTanh\n",
      "creating: createSigmoid\n",
      "creating: createGRU\n",
      "creating: createSelect\n",
      "creating: createLinear\n",
      "creating: createDropout\n",
      "creating: createReLU\n",
      "creating: createLinear\n",
      "creating: createSigmoid\n",
      "creating: createBCECriterion\n",
      "creating: createMaxEpoch\n",
      "creating: createAdam\n",
      "creating: createDistriOptimizer\n",
      "creating: createEveryEpoch\n",
      "creating: createTop1Accuracy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/bigdl/dllib/optim/optimizer.py:864: UserWarning: You are recommended to use `create` method to create an optimizer.\n",
      "  warnings.warn(\"You are recommended to use `create` method to create an optimizer.\")\n"
     ]
    }
   ],
   "source": [
    "from bigdl.dllib.optim.optimizer import *\n",
    "from bigdl.dllib.nn.criterion import *\n",
    "\n",
    "# max_epoch = 4\n",
    "max_epoch = 1\n",
    "batch_size = 4\n",
    "model_type = 'gru'\n",
    "\n",
    "\n",
    "optimizer = Optimizer(\n",
    "        model=build_model(w2v),\n",
    "        training_rdd=train_rdd,\n",
    "        criterion=BCECriterion(),\n",
    "        end_trigger=MaxEpoch(max_epoch),\n",
    "        batch_size=batch_size,\n",
    "        optim_method=Adam())\n",
    "\n",
    "optimizer.set_validation(\n",
    "        batch_size=batch_size,\n",
    "        val_rdd=test_rdd,\n",
    "        trigger=EveryEpoch(),\n",
    "        val_method=Top1Accuracy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the training process be visualized by TensorBoard, training summaries should be saved as a format of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTrainSummary\n",
      "creating: createSeveralIteration\n",
      "creating: createValidationSummary\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.dllib.optim.optimizer.Optimizer at 0x7f5d3cdc65c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "logdir = '/tmp/.bigdl/'\n",
    "app_name = 'adam-' + dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_summary = TrainSummary(log_dir=logdir, app_name=app_name)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "val_summary = ValidationSummary(log_dir=logdir, app_name=app_name)\n",
    "optimizer.set_train_summary(train_summary)\n",
    "optimizer.set_val_summary(val_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Done.\n",
      "CPU times: user 91.6 ms, sys: 14.8 ms, total: 106 ms\n",
      "Wall time: 18min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_model = optimizer.optimize()\n",
    "print (\"Optimization Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "Validation accuracy is shown in the training log, here let's get the accuracy on validation set by hand.\n",
    "\n",
    "Predict the `test_rdd` (validation set data), and obtain the predicted label and ground truth label in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train_model.predict(test_rdd)\n",
    "\n",
    "def map_predict_label(l):\n",
    "    if l > 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def map_groundtruth_label(l):\n",
    "    return l.to_ndarray()[0]\n",
    "\n",
    "y_pred = np.array([ map_predict_label(s) for s in predictions.collect()])\n",
    "\n",
    "y_true = np.array([map_groundtruth_label(s.label) for s in test_rdd.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's see the prediction accuracy on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy on validation set is:  0.89096\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(0, y_pred.size):\n",
    "    if (y_pred[i] == y_true[i]):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = float(correct) / y_pred.size\n",
    "print ('Prediction accuracy on validation set is: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ding/anaconda3/envs/conda_py36/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['Normalize']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.use('Agg')\n",
    "%pylab inline \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "cm.shape\n",
    "\n",
    "df_cm = pd.DataFrame(cm)\n",
    "plt.figure(figsize = (5,4))\n",
    "sn.heatmap(df_cm, annot=True,fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because of the limitation of ariticle length, not all the results of optional models can be shown respectively. Please try other provided optional models to see the results. If you are interested in optimizing the results, try different training parameters which may make inpacts on the result, such as the max sequence length, batch size, training epochs, preprocessing schemes, optimization methods and so on. Among the models, CNN training would be much quicker. Note that the LSTM and it variants (eg. GRU) are difficult to train, even a unsuitable batch size may cause the model not converge. In addition it is prone to overfitting, please try different dropout threshold and/or add regularizers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
