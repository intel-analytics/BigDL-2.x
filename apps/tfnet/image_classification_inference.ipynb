{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using tensorflow pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow-Slim image classification model library provides both the implementation and pre-trianed checkpoint many popular convolution nerual nets for image classification.\n",
    "\n",
    "Using TFNet in Analytics-Zoo, we can easily load these pre-trained model and make distributed inference with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the slim image classification model library to $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path\n",
    "slim_path = \"/tmp/tensorflow/models/research/slim\" # Please set this to the directory where you clone the tensorflow models repository\n",
    "sys.path.append(slim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the inference graph and restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dataset_factory\n",
    "from nets import inception\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image = tf.random_uniform(shape=[224, 224, 3])\n",
    "batch_size_tensor = tf.placeholder_with_default(32, shape=())\n",
    "images = tf.train.batch(\n",
    "    [image],\n",
    "    batch_size=batch_size_tensor,\n",
    "    num_threads=4,\n",
    "    capacity=5 * 32)\n",
    "# One thing to note is that the batch_size must be placeholder, so that batch size is not hard coded into graph and analytics-zoo can work on different batch sizes.\n",
    "# The above code can be replaced with a single line of code\n",
    "# images = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3))\n",
    "\n",
    "with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "    logits, end_points = inception.inception_v1(images, num_classes=1001, is_training=False)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/tmp/models/inception_v1.ckpt\") # You need to edit this path to the checkpoint you downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the graph as a frozen inference graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export_tf utility function will frozen the tensorflow graph, strip unused operation according to the inputs and outputs and save it to the specified directory along with the input/output tensor names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 230 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from zoo.util.tf import export_tf\n",
    "export_tf(sess, \"/tmp/models/tfnet\", inputs=[images], outputs=[logits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Analytics-Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTFNet\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.net import TFNet\n",
    "model = TFNet.from_export_folder(\"/tmp/models/tfnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it on one image\n",
    "\n",
    "![Test Image](./test.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "im = cv2.imread(\"test.jpg\")\n",
    "im = cv2.resize(im, (224, 224))\n",
    "im = (im - 127.0) / 128.0\n",
    "im = np.expand_dims(im, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persian_cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "result = model.predict([im])\n",
    "print(idx2label[np.argmax(result, 1)[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
