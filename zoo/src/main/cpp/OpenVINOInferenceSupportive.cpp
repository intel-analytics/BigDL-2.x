/*
 * Copyright 2018 Analytics Zoo Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

#include <functional>
#include <iostream>
#include <fstream>
#include <random>
#include <string>
#include <memory>
#include <vector>
#include <time.h>
#include <limits>
#include <chrono>
#include <algorithm>

#include <common/samples/slog.hpp>

#include <inference_engine/inference_engine.hpp>
#include <extension/ext_list.hpp>

#include "OpenVINOInferenceSupportive.hpp"

using namespace InferenceEngine;

OpenVINOInferenceSupportive::OpenVINOInferenceSupportive()
{
    slog::info << "InferenceEngine: " << GetInferenceEngineVersion() << "\n";
}

ExecutableNetwork* OpenVINOInferenceSupportive::loadOpenVINOIR(const std::string modelFilePath, const std::string weightFilePath, const int deviceType)
{
    try
    {
        slog::info << "InferenceEngine: " << GetInferenceEngineVersion() << "\n";

        // ---------------------------  Load Plugin for inference engine -------------------------------------
        slog::info << "Loading plugin" << slog::endl;

        /*If CPU device, load default library with extensions that comes with the product*/
        InferencePlugin plugin;
        if (deviceType == OpenVINODeviceName::CPU)
        {
            /**
                * cpu_extensions library is compiled from "extension" folder containing
                * custom MKLDNNPlugin layer implementations. These layers are not supported
                * by mkldnn, but they can be useful for inferring custom topologies.
                **/
            plugin = PluginDispatcher({"", "../../../lib/intel64", ""}).getPluginByDevice("CPU");
            plugin.AddExtension(std::make_shared<Extensions::Cpu::CpuExtensions>());
        }
        //printPluginVersion(plugin, std::cout);
        // -----------------------------------------------------------------------------------------------------

        // ---------------------------  Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
        slog::info << "Loading network files:"
                      "\n\t"
                   << modelFilePath << "\n\t" << weightFilePath << slog::endl;

        CNNNetReader networkReader;
        /** Read network model **/
        networkReader.ReadNetwork(modelFilePath);

        /** Extract model name and load weights **/
        networkReader.ReadWeights(weightFilePath);
        CNNNetwork network = networkReader.getNetwork();
        // -----------------------------------------------------------------------------------------------------

        InputsDataMap inputsInfo(network.getInputsInfo());
        InputInfo::Ptr inputInfo = inputsInfo.begin()->second;

        // ---------------------------  Loading model to the plugin ------------------------------------------
        slog::info << "Loading model to the plugin" << slog::endl;
        ExecutableNetwork executable_network = plugin.LoadNetwork(network, {});
        // -----------------------------------------------------------------------------------------------------

        ExecutableNetwork* pexecutableNetwork = new ExecutableNetwork(executable_network);
        return pexecutableNetwork;
    }
    catch (const std::exception &error)
    {
        slog::err << error.what() << slog::endl;
        exit(1);
    }
    catch (...)
    {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        exit(1);
    }
}

CTensor<float> OpenVINOInferenceSupportive::predict(ExecutableNetwork executable_network, CTensor<float> datatensor)
{
    try
    {
        // ---------------------------  Create infer request -------------------------------------------------
        InferRequest infer_request = executable_network.CreateInferRequest();
        ConstInputsDataMap inputsInfo(executable_network.GetInputsInfo());
        ConstOutputsDataMap outputsInfo(executable_network.GetOutputsInfo());
        // ---------------------------------------------------------------------------------------------------

        // ---------------------------  Prepare input --------------------------------------------------------
        std::string imageInputName;

        // To do list: 1. ConstInputsDataMap with multiple map member.
        auto inputInfobegin = inputsInfo.begin();

        imageInputName = inputInfobegin->first;
        auto inputInfo = inputInfobegin->second;

        /** Creating input blob **/
        Blob::Ptr inputBlob = infer_request.GetBlob(imageInputName);

        // Check data integrity
        if (inputBlob->getTensorDesc().getDims() != datatensor.shape)
        {
            slog::info << "Data Shape Mismatched!" << slog::endl;
            return CTensor<float>(NULL, std::vector<size_t>(0));
        }

        float *inputBuffer = static_cast<float *>(inputBlob->buffer());

        /** Iterate over all input data element **/
        for (size_t data_id = 0; data_id < datatensor.data_size; ++data_id)
        {
            inputBuffer[data_id] = datatensor.data[data_id];
        }

        // -----------------------------------------------------------------------------------------------------

        // ---------------------------   Do inference  ---------------------------------------------------------
        slog::info << "Start inference (" << 1 << " iterations)" << slog::endl;

        typedef std::chrono::high_resolution_clock Time;
        typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;
        typedef std::chrono::duration<float> fsec;

        double total = 0.0;
        /** Start inference & calc performance **/
        auto t0 = Time::now();
        infer_request.Infer();
        auto t1 = Time::now();
        fsec fs = t1 - t0;
        ms d = std::chrono::duration_cast<ms>(fs);
        total += d.count();
        // -----------------------------------------------------------------------------------------------------

        std::string outputName;

        auto outputInfobegin = outputsInfo.begin();

        outputName = outputInfobegin->first;
        auto outputInfo = outputInfobegin->second;

        const Blob::Ptr outputBlob = infer_request.GetBlob(outputName);
        float *detection = static_cast<PrecisionTrait<Precision::FP32>::value_type *>(outputBlob->buffer());

        std::vector<size_t> resultshape = outputBlob->getTensorDesc().getDims();
        CTensor<float> result_tensor(resultshape);

        for (size_t data_id = 0; data_id < result_tensor.data_size; ++data_id)
        {
            result_tensor.data[data_id] = detection[data_id];
        }

        std::cout << std::endl
                  << "total inference time: " << total << std::endl;
        std::cout << "Average running time of one iteration: " << total / static_cast<double>(1) << " ms" << std::endl;
        std::cout << std::endl
                  << "Throughput: " << 1000 * static_cast<double>(1) * 1 / total << " FPS" << std::endl;
        std::cout << std::endl;

        return result_tensor;
    }
    catch (const std::exception &error)
    {
        slog::err << error.what() << slog::endl;
        exit(1);
    }
    catch (...)
    {
        slog::err << "Unknown/internal exception happened." << slog::endl;
        exit(1);
    }
}