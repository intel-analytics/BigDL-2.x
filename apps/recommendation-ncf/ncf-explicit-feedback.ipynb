{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCF Recommender with Explict Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  /home/juimdpp/spark/examples/target/scala-2.11/jars/spark-examples_2.11-2.4.4.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demostrate how to build a neural network recommendation system, Neural Collaborative Filtering(NCF) with explict feedback. We use Recommender API in Analytics Zoo to build a model, and use optimizer of BigDL to train the model. \n",
    "\n",
    "The system ([Recommendation systems: Principles, methods and evaluation](http://www.sciencedirect.com/science/article/pii/S1110866515000341)) normally prompts the user through the system interface to provide ratings for items in order to construct and improve the model. The accuracy of recommendation depends on the quantity of ratings provided by the user.  \n",
    "\n",
    "NCF([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)) leverages a multi-layer perceptrons to learn the userâ€“item interaction function, at the mean time, NCF can express and generalize matrix factorization under its framework. includeMF(Boolean) is provided for users to build a NCF with or without matrix factorization. \n",
    "\n",
    "Data: \n",
    "* The dataset we used is movielens-1M ([link](https://grouplens.org/datasets/movielens/1m/)), which contains 1 million ratings from 6000 users on 4000 movies.  There're 5 levels of rating. We will try classify each (user,movie) pair into 5 classes and evaluate the effect of algortithms using Mean Absolute Error.  \n",
    "  \n",
    "References: \n",
    "* A Keras implementation of Movie Recommendation([notebook](https://github.com/ririw/ririw.github.io/blob/master/assets/Recommending%20movies.ipynb)) from the [blog](http://blog.richardweiss.org/2016/09/25/movie-embeddings.html).\n",
    "* Nerual Collaborative filtering ([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/bigdl/util/engine.py:41: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /home/juimdpp/spark, and pyspark is found in: /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, please use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/zoo/util/engine.py:42: UserWarning: Find both SPARK_HOME and pyspark. You may need to check whether they match with each other. SPARK_HOME environment variable is set to: /home/juimdpp/spark, and pyspark is found in: /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/pyspark/__init__.py. If they are unmatched, you are recommended to use one source only to avoid conflict. For example, you can unset SPARK_HOME and use pyspark only.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepending /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/bigdl/share/conf/spark-bigdl.conf to sys.path\n",
      "Adding /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/zoo/share/lib/analytics-zoo-bigdl_0.12.2-spark_2.4.3-0.10.0-jar-with-dependencies.jar to BIGDL_JARS\n",
      "Prepending /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/zoo/share/conf/spark-analytics-zoo.conf to sys.path\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.keras.layers import *\n",
    "from zoo.models.recommendation import UserItemFeature\n",
    "from zoo.models.recommendation import NeuralCF\n",
    "from zoo.common.nncontext import init_nncontext\n",
    "import matplotlib\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter\n",
    "from bigdl.dataset import movielens\n",
    "from bigdl.util.common import *\n",
    "\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initilaize NN context, it will get a SparkContext with optimized configuration for BigDL performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN_CONTEXT\n",
      "GETORCREATE\n",
      "INIT\n",
      "dict_items([('spark.shuffle.reduceLocality.enabled', 'false'), ('spark.shuffle.blockTransferService', 'nio'), ('spark.scheduler.minRegisteredResourcesRatio', '1.0'), ('spark.scheduler.maxRegisteredResourcesWaitingTime', '3600s'), ('spark.speculation', 'false'), ('spark.serializer', 'org.apache.spark.serializer.JavaSerializer')])\n",
      "pyspark_submit_args is:  --driver-class-path /home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/zoo/share/lib/analytics-zoo-bigdl_0.12.2-spark_2.4.3-0.10.0-jar-with-dependencies.jar:/home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/bigdl/share/lib/bigdl-0.12.2-jar-with-dependencies.jar pyspark-shell \n",
      "GETORCREATE\n"
     ]
    }
   ],
   "source": [
    "sc = init_nncontext(\"NCF Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executorEnv.OMP_NUM_THREADS', '1'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.JavaSerializer'),\n",
       " ('spark.eventLog.dir', 'file:///tmp/spark-events'),\n",
       " ('spark.shuffle.reduceLocality.enabled', 'false'),\n",
       " ('spark.shuffle.blockTransferService', 'nio'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'anna-01'),\n",
       " ('spark.executorEnv.KMP_BLOCKTIME', '0'),\n",
       " ('spark.driver.port', '42263'),\n",
       " ('spark.executorEnv.KMP_AFFINITY', 'granularity=fine,compact,1,0'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.speculation', 'false'),\n",
       " ('spark.history.fs.logDirectory', 'file:///tmp/spark-events'),\n",
       " ('spark.scheduler.maxRegisteredResourcesWaitingTime', '3600s'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.scheduler.minRegisteredResourcesRatio', '1.0'),\n",
       " ('spark.app.name', 'NCF Example'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/zoo/share/lib/analytics-zoo-bigdl_0.12.2-spark_2.4.3-0.10.0-jar-with-dependencies.jar:/home/juimdpp/anaconda3/envs/venv/lib/python3.7/site-packages/bigdl/share/lib/bigdl-0.12.2-jar-with-dependencies.jar'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1625711275118'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.executorEnv.KMP_SETTINGS', '1')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.defaultParallelism\n",
    "sc._conf.getAll()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and read movielens 1M data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T08:13:08.071512Z",
     "start_time": "2017-08-03T08:13:04.077308Z"
    }
   },
   "outputs": [],
   "source": [
    "# movielens_data = movielens.get_id_ratings(\"/tmp/movielens/\")\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1 1193    5]\n",
      " [   1  661    3]\n",
      " [   1  914    3]\n",
      " ...\n",
      " [6040  562    5]\n",
      " [6040 1096    4]\n",
      " [6040 1097    4]]\n",
      "<class 'numpy.ndarray'>\n",
      "24005016\n"
     ]
    }
   ],
   "source": [
    "print(movielens_data)\n",
    "print(type(movielens_data))\n",
    "print(movielens_data.size * movielens_data.itemsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand the data. Each record is in format of (userid, movieid, rating_score). UserIDs range between 1 and 6040. MovieIDs range between 1 and 3952. Ratings are made on a 5-star scale (whole-star ratings only). Counts of users and movies are recorded for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T08:13:08.082461Z",
     "start_time": "2017-08-03T08:13:08.075167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 3)\n",
      "1 6040 1 3952 [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "min_user_id = np.min(movielens_data[:,0])\n",
    "max_user_id = np.max(movielens_data[:,0])\n",
    "min_movie_id = np.min(movielens_data[:,1])\n",
    "max_movie_id = np.max(movielens_data[:,1])\n",
    "rating_labels= np.unique(movielens_data[:,2])\n",
    "\n",
    "print(movielens_data.shape)\n",
    "print(min_user_id, max_user_id, min_movie_id, max_movie_id, rating_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T03:45:25.657796Z",
     "start_time": "2017-08-03T03:45:25.650852Z"
    }
   },
   "source": [
    "Transform original data into RDD of sample. \n",
    "We use optimizer of BigDL directly to train the model, it requires data to be provided in format of RDD([Sample](https://bigdl-project.github.io/master/#APIGuide/Data/#sample)). A `Sample` is a BigDL data structure which can be constructed using 2 numpy arrays, `feature` and `label` respectively. The API interface is `Sample.from_ndarray(feature, label)`\n",
    "Here, labels are tranformed into zero-based since original labels start from 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T03:47:57.601392Z",
     "start_time": "2017-08-03T03:47:57.591491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<zoo.models.recommendation.recommender.UserItemFeature at 0x7f09f2bf4c90>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7f0a85e69150>,\n",
       " <zoo.models.recommendation.recommender.UserItemFeature at 0x7f09fea76410>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_sample(user_id, item_id, rating):\n",
    "    sample = Sample.from_ndarray(np.array([user_id, item_id]), np.array([rating]))\n",
    "    return UserItemFeature(user_id, item_id, sample)\n",
    "pairFeatureRdds = sc.parallelize(movielens_data)\\\n",
    "    .map(lambda x: build_sample(x[0], x[1], x[2]-1))\n",
    "pairFeatureRdds.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T03:26:59.143086Z",
     "start_time": "2017-08-03T03:26:59.136374Z"
    }
   },
   "source": [
    "Randomly split the data into train (80%) and validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T08:13:10.225186Z",
     "start_time": "2017-08-03T08:13:10.117969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainPairFeatureRdds, valPairFeatureRdds = pairFeatureRdds.randomSplit([0.8, 0.2], seed= 1)\n",
    "valPairFeatureRdds.cache()\n",
    "train_rdd= trainPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "val_rdd= valPairFeatureRdds.map(lambda pair_feature: pair_feature.sample)\n",
    "val_rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Analytics Zoo, it is simple to build NCF model by calling NeuralCF API. You need specify the user count, item count and class number according to your data, then add hidden layers as needed, you can also choose to include matrix factorization in the network. The model could be fed into an Optimizer of BigDL or NNClassifier of analytics-zoo. Please refer to the document for more details. In this example, we demostrate how to use optimizer of BigDL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T09:39:03.502185Z",
     "start_time": "2017-08-03T09:39:03.477423Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEYY\n",
      "creating: createZooKerasInput\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasSelect\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@4a15823\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@67596b1c\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasSelect\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@69e6c875\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@367655ef\n",
      "creating: createZooKerasEmbedding\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@17450ede\n",
      "creating: createZooKerasEmbedding\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@58d59c40\n",
      "creating: createZooKerasFlatten\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@2e327679\n",
      "creating: createZooKerasFlatten\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@37ee15aa\n",
      "creating: createZooKerasMerge\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@7e3b11b9\n",
      "creating: createZooKerasDense\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@441e5b70\n",
      "creating: createZooKerasDense\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@378683db\n",
      "creating: createZooKerasDense\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result com.intel.analytics.zoo.pipeline.api.autograd.Variable@57c33fcf\n",
      "MODEL\n",
      "creating: createZooKerasModel\n",
      "creating: createZooNeuralCF\n"
     ]
    }
   ],
   "source": [
    "ncf = NeuralCF(user_count=max_user_id, \n",
    "               item_count=max_movie_id, \n",
    "               class_num=5, \n",
    "               hidden_layers=[20, 10], \n",
    "               include_mf = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n",
    "\n",
    "Compile model given specific optimizers, loss, as well as metrics for evaluation. Optimizer tries to minimize the loss of the neural net with respect to its weights/biases, over the training set. To create an Optimizer in BigDL, you want to at least specify arguments: model(a neural network model), criterion(the loss function), traing_rdd(training dataset) and batch size. Please refer to ([ProgrammingGuide](https://bigdl-project.github.io/master/#ProgrammingGuide/optimization/))and ([Optimizer](https://bigdl-project.github.io/master/#APIGuide/Optimizers/Optimizer/)) for more details to create efficient optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T09:39:07.359788Z",
     "start_time": "2017-08-03T09:39:07.321748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP\n",
      "creating: createAdam\n",
      "creating: createZooKerasSparseCategoricalCrossEntropy\n",
      "creating: createZooKerasSparseCategoricalAccuracy\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result None\n"
     ]
    }
   ],
   "source": [
    "ncf.compile(optimizer= \"adam\",\n",
    "            loss= \"sparse_categorical_crossentropy\",\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect logs\n",
    "\n",
    "You can leverage tensorboard to see the summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result None\n"
     ]
    }
   ],
   "source": [
    "tmp_log_dir = create_tmp_path()\n",
    "ncf.set_tensorboard(tmp_log_dir, \"training_ncf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-04T02:28:52.344745Z",
     "start_time": "2017-08-04T02:28:52.274699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIT\n",
      "CALLZOOFUNC\n",
      "com.intel.analytics.zoo.tfpark.python.PythonTFPark@7509b72b\n",
      "com.intel.analytics.zoo.pipeline.nnframes.python.PythonNNFrames@798fd45d\n",
      "com.intel.analytics.zoo.feature.python.PythonImageFeature@334b2238\n",
      "com.intel.analytics.zoo.pipeline.api.keras.python.PythonAutoGrad@1612b3b0\n",
      "result None\n"
     ]
    }
   ],
   "source": [
    "ncf.fit(train_rdd, \n",
    "        nb_epoch= 3, \n",
    "        batch_size= 120000, \n",
    "        validation_data=val_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Zoo models make inferences based on the given data using model.predict(val_rdd) API. A result of RDD is returned. predict_class returns the predicted label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ncf.predict(val_rdd)\n",
    "results.take(5)\n",
    "\n",
    "results_class = ncf.predict_class(val_rdd)\n",
    "results_class.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Analytics Zoo, Recommender has provied 3 unique APIs to predict user-item pairs and make recommendations for users or items given candidates.\n",
    "Predict for user item pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "userItemPairPrediction = ncf.predict_user_item_pair(valPairFeatureRdds)\n",
    "for result in userItemPairPrediction.take(5): print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend 3 items for each user given candidates in the feature RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "userRecs = ncf.recommend_for_user(valPairFeatureRdds, 3)\n",
    "for result in userRecs.take(5): print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend 3 users for each item given candidates in the feature RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemRecs = ncf.recommend_for_item(valPairFeatureRdds, 3)\n",
    "for result in itemRecs.take(5): print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the train and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-03T09:50:45.229520Z",
     "start_time": "2017-08-03T09:50:44.643083Z"
    }
   },
   "outputs": [],
   "source": [
    "#retrieve train and validation summary object and read the loss data into ndarray's. \n",
    "train_loss = np.array(ncf.get_train_summary(\"Loss\"))\n",
    "val_loss = np.array(ncf.get_validation_summary(\"Loss\"))\n",
    "#plot the train and validation curves\n",
    "# each event data is a tuple in form of (iteration_count, value, timestamp)\n",
    "plt.figure(figsize = (12,6))\n",
    "plt.plot(train_loss[:,0],train_loss[:,1],label='train loss')\n",
    "plt.plot(val_loss[:,0],val_loss[:,1],label='val loss',color='green')\n",
    "plt.scatter(val_loss[:,0],val_loss[:,1],color='green')\n",
    "plt.legend();\n",
    "plt.xlim(0,train_loss.shape[0]+10)\n",
    "plt.grid(True)\n",
    "plt.title(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "top1 = np.array(ncf.get_validation_summary(\"Top1Accuracy\"))\n",
    "plt.plot(top1[:,0],top1[:,1],label='top1')\n",
    "plt.title(\"top1 accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "141px",
    "width": "251px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}