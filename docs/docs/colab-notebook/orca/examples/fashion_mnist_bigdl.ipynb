{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fashion_mnist_bigdl.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjOxxiX5mvg/NYvHmWTr+b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sgwhat/analytics-zoo/blob/colab/docs/docs/colab-notebook/orca/examples/fashion_mnist_bigdl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP3kaOu8WsNL"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ResW0PSY4xWC"
      },
      "source": [
        "## **Environment Preparation**\n",
        "\n",
        "**Install Java 8**\n",
        "\n",
        "Run the cell on the **Google Colab** to install jdk 1.8.\n",
        "\n",
        "**Note:** if you run this notebook on your computer, root permission is required when running the cell to install Java 8. (You may ignore this cell if Java 8 has already been set up in your computer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0p4mAbe0iH4"
      },
      "source": [
        "# Install jdk8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "import os\n",
        "# Set environment variable JAVA_HOME.\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
        "!java -version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-dVmMas8KfK"
      },
      "source": [
        "**Install Analytics Zoo**\n",
        "\n",
        "[Conda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) is needed to prepare the Python environment for running this example. \n",
        "\n",
        "**Note**: The following code cell is specific for setting up conda environment on Colab; for general conda installation, please refer to the [install guide](https://docs.conda.io/projects/conda/en/latest/user-guide/install/) for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69suYs9K0kiu"
      },
      "source": [
        "import sys\n",
        "\n",
        "# Get current python version\n",
        "version_info = sys.version_info\n",
        "python_version = f\"{version_info.major}.{version_info.minor}.{version_info.micro}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JczTAk5E0nW2"
      },
      "source": [
        "# Install Miniconda\n",
        "!wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-4.5.4-Linux-x86_64.sh\n",
        "!./Miniconda3-4.5.4-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Update Conda\n",
        "!conda install --channel defaults conda python=$python_version --yes\n",
        "!conda update --channel defaults --all --yes\n",
        "\n",
        "# Append to the sys.path\n",
        "_ = (sys.path\n",
        "        .append(f\"/usr/local/lib/python{version_info.major}.{version_info.minor}/site-packages\"))\n",
        "\n",
        "os.environ['PYTHONHOME']=\"/usr/local\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaopMtV98hB-"
      },
      "source": [
        "You can install the latest pre-release version using `pip install --pre  analytics-zoo`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUGTC8lT07ZE"
      },
      "source": [
        "# Install latest pre-release version of Analytics Zoo \n",
        "# Installing Analytics Zoo from pip will automatically install pyspark, bigdl, and their dependencies.\n",
        "!pip install --pre --upgrade analytics-zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EW8lPzz0_i0"
      },
      "source": [
        "# Install python dependencies\n",
        "!pip install torch==1.7.1 torchvision==0.8.2\n",
        "!pip install jep==3.9.0\n",
        "!pip install cloudpickle\n",
        "!pip install argparse\n",
        "!pip install tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mo8uojl9rQN"
      },
      "source": [
        "## **BigDL using Orca APIs**\n",
        "\n",
        "In this guide we will describe how to scale out standard (single node) PyTorch programs using Orca and visualize the results with Tensorboard in 5 simple steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-mCzxMaN-Fc"
      },
      "source": [
        "#import necessary libraries and modules\n",
        "from __future__ import print_function\n",
        "\n",
        "from zoo.orca import init_orca_context, stop_orca_context\n",
        "from zoo.orca import OrcaContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrFNoK26_Dzp"
      },
      "source": [
        "### **Step 1: Init Orca Context**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXSqnMoQ24PO"
      },
      "source": [
        "# recommended to set it to True when running Analytics Zoo in Jupyter notebook. \n",
        "OrcaContext.log_output = True # (this will display terminal's stdout and stderr in the Jupyter notebook).\n",
        "\n",
        "cluster_mode = \"local\"\n",
        "\n",
        "if cluster_mode == \"local\":\n",
        "    init_orca_context(cores=1, memory=\"2g\") # run in local mode\n",
        "elif cluster_mode == \"k8s\":\n",
        "    init_orca_context(cluster_mode=\"k8s\", num_nodes=2, cores=4) # run on K8s cluster\n",
        "elif cluster_mode == \"yarn\":\n",
        "    init_orca_context(\n",
        "        cluster_mode=\"yarn-client\", cores=4, num_nodes=2, memory=\"2g\",\n",
        "        driver_memory=\"10g\", driver_cores=1,\n",
        "        conf={\"spark.rpc.message.maxSize\": \"1024\",\n",
        "              \"spark.task.maxFailures\": \"1\",\n",
        "              \"spark.driver.extraJavaOptions\": \"-Dbigdl.failure.retryTimes=1\"}) # run on Hadoop YARN cluster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUOk_SJRSfbD"
      },
      "source": [
        "This is the only place where you need to specify local or distributed mode. View [Orca Context](https://analytics-zoo.readthedocs.io/en/latest/doc/Orca/Overview/orca-context.html) for more details.\n",
        "\n",
        "**Note**: You should export HADOOP_CONF_DIR=/path/to/hadoop/conf/dir when you run on Hadoop YARN cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6q-5q_H_ZFc"
      },
      "source": [
        "### **Step 2: Define the Model**\n",
        "You may define your model, loss and optimizer in the same way as in any standard (single node) PyTorch program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysq9FGKUkfax"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = Net()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6__RXzi5hwT"
      },
      "source": [
        "# training loss vs. epochs\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "batch_size = 32\n",
        "epochs = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PR_6N61wADCI"
      },
      "source": [
        "### **Step 3: Define Train Dataset**\n",
        "\n",
        "You can define the dataloader using standarad `torch.utils.data.DataLoader` and put into dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm9VVkHlWl2d"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# create train_dataloader \n",
        "trainset = torchvision.datasets.FashionMNIST('./data', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "# create test_dataloader\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKvWhWLrDYPp"
      },
      "source": [
        "### **Step 4: Fit with Orca Estimator**\n",
        "\n",
        "First, Create an Estimator and set its backend to `bigdl`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd3Mpbz_YRWu"
      },
      "source": [
        "from zoo.orca.learn.pytorch import Estimator\n",
        "from zoo.orca.learn.metrics import Accuracy\n",
        "\n",
        "orca_estimator = Estimator.from_torch(model=net, optimizer=optimizer, loss=criterion, metrics=[Accuracy()], backend=\"bigdl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGjNwF-eKBI7"
      },
      "source": [
        "To load training results for Tensorboard, set the log output path and application name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp8hzltsZCQP"
      },
      "source": [
        "# the path of the directory where to save the log files to be parsed by TensorBoard\n",
        "tensorboard_dir = \"runs\"\n",
        "# \"bigdl\" is the application name for tensorboard to save training and validation results under log path\n",
        "orca_estimator.set_tensorboard(tensorboard_dir, \"bigdl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8QZ2RzQYiUk"
      },
      "source": [
        "Next, fit using the Estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7YBJYB7YpTM"
      },
      "source": [
        "from zoo.orca.learn.trigger import EveryEpoch\n",
        "\n",
        "orca_estimator.fit(data=trainloader, epochs=epochs, validation_data=testloader, checkpoint_trigger=EveryEpoch())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH4dg0b2Yx61"
      },
      "source": [
        "Finally, evaluate using the Estimator. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WwT6OHJY4oO"
      },
      "source": [
        "res = orca_estimator.evaluate(data=testloader)\n",
        "print(\"Accuracy of the network on the test images: %s\" % res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl31-MSZiyP6"
      },
      "source": [
        "The accuracy of this model has reached 74%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSVS2zHD5Sfs"
      },
      "source": [
        "# stop orca context when program finishes\n",
        "stop_orca_context()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OBlnkPjM80S"
      },
      "source": [
        "### **Step 5: Visualization by Tensorboard**\n",
        "\n",
        "TensorBoard is a visualization toolkit for machine learning experimentation. TensorBoard allows tracking and visualizing metrics such as loss and accuracy, visualizing the model graph, viewing histograms, displaying images and much more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA2wmwizYv1B"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# helper function to show an image\n",
        "def matplotlib_imshow(img, one_channel=False):\n",
        "    if one_channel:\n",
        "        img = img.mean(dim=0)\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    if one_channel:\n",
        "        plt.imshow(npimg, cmap=\"Greys\")\n",
        "    else:\n",
        "        plt.imshow(np.transpose(npimg, (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lNnhkX68cqW"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adXtrlX61ayB"
      },
      "source": [
        "A brief overview of the dashboards shown (tabs in top navigation bar):\n",
        "\n",
        "* The **SCALARS** dashboard shows how the loss and metrics change with every epoch. You can use it to also track training speed, learning rate, and other scalar values.\n",
        "* The **IMAGES** dashboard can  be extremely helpful to sample and examine your input data, or to visualize layer weights and generated tensors. \n",
        "* The **GRAPHS** dashboard helps you visualize your model. In this case, the graph of layers is shown which can help you ensure it is built correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK51zoum28FI"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#Before logging anything, we need to create a SummaryWriter instance.\n",
        "writer = SummaryWriter(tensorboard_dir + '/bigdl/train')\n",
        "\n",
        "#constant for classes\n",
        "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
        "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')\n",
        "\n",
        "# plot some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# show images\n",
        "matplotlib_imshow(img_grid, one_channel=True)\n",
        "\n",
        "# write to tensorboard\n",
        "writer.add_image('fashion_mnist_images', img_grid)\n",
        "\n",
        "# inspect the model using tensorboard\n",
        "writer.add_graph(net, images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSctDHMnxRue"
      },
      "source": [
        "If you do not need the summary writer anymore, call close() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om95R_pPZ9Xt"
      },
      "source": [
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndEszjjuw6HF"
      },
      "source": [
        "Start TensorBoard, specifying the root log directory you used above. \n",
        "Argument ``logdir`` points to directory where TensorBoard will look to find \n",
        "event files that it can display. TensorBoard will recursively walk \n",
        "the directory structure rooted at logdir, looking for .*tfevents.* files.\n",
        "This dashboard shows how the loss and accuracy change with every iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhQsdAhLEQxa"
      },
      "source": [
        "# move into logdata file for tensorboard using\n",
        "%tensorboard --logdir '/content/runs/bigdl/train'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}