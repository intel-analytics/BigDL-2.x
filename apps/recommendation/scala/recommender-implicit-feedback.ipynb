{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation with Implicit Feedback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demostrate how to build neural network recommendation system with implicit feedback, which indirectly reflects usersâ€™ preference through behaviours like watching videos, purchasing products and clicking items. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start BigDL engine and spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.intel.analytics.bigdl._\n",
    "import com.intel.analytics.bigdl.nn._\n",
    "import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "import com.intel.analytics.bigdl.optim._\n",
    "import com.intel.analytics.bigdl.utils.Engine\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.ml.{DLClassifier, DLModel}\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val conf = Engine.createSparkConf()\n",
    "val spark = SparkSession.builder().master(\"local[8]\").appName(\"RecommendationImplicitExample\").config(conf).getOrCreate()\n",
    "Logger.getLogger(\"org\").setLevel(Level.ERROR)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "import spark.implicits._\n",
    "Engine.init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load data and check data quality. Here, we assume it is downloaded from [link](https://grouplens.org/datasets/movielens/1m/) and put it into directory of \"/tmp/movielens/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "// mvpath is the location of downloaded data\n",
    "val mvpath = \"/tmp/movielens/ml-1m\"\n",
    "\n",
    "def getDataDF = {\n",
    "    val indexedDF = spark.read.text(mvpath + \"/ratings.dat\").as[String]\n",
    "      .map(x => {\n",
    "        val data: Array[Double] = x.split(\"::\").map(n => n.toDouble)\n",
    "        (data(0), data(1), data(2))\n",
    "      })\n",
    "      .toDF(\"userIdIndex\",\"itemIdIndex\",\"label\")\n",
    "    \n",
    "    val minMaxRow = indexedDF.agg(min(\"userIdIndex\"), max(\"userIdIndex\"), min(\"itemIdIndex\"), max(\"itemIdIndex\")).collect()(0)\n",
    "\n",
    "    val minUserId = minMaxRow.getDouble(0)\n",
    "    val userCount = minMaxRow.getDouble(1)\n",
    "    val minMovieId = minMaxRow.getDouble(2)\n",
    "    val itemCount = minMaxRow.getDouble(3)\n",
    "    (indexedDF, userCount, itemCount)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----+\n",
      "|userIdIndex|itemIdIndex|label|\n",
      "+-----------+-----------+-----+\n",
      "|        1.0|     1193.0|  5.0|\n",
      "|        1.0|      661.0|  3.0|\n",
      "|        1.0|      914.0|  3.0|\n",
      "+-----------+-----------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "userCount: 6040.0\n",
      "itemCount: 3952.0\n"
     ]
    }
   ],
   "source": [
    "val (indexedDF, userCount, itemCount) = getDataDF\n",
    "indexedDF.show(3)\n",
    "println(\"userCount: \" + userCount + \"\\nitemCount: \" + itemCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We focus on impicit feedback, the user-item interaction is defined as 1 if interaction, 0 otherwise.  Ratings are all trasformed into 1.0 and negative samples are added by randomly sampling from the whole user and item space, then prepare features into label points required by DLClassifer and DLModel.\n",
    "* Then split data into training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.util.Random\n",
    "import org.apache.spark.ml.feature.{LabeledPoint, StringIndexer}\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "\n",
    "/* Negative samples are needed in this use case. \n",
    " addNegativeSample is defined to add negative samples by randomly sampling from the whole user and item space.\n",
    "*/\n",
    "def addNegativeSample(indexedDF: DataFrame) = {\n",
    "    val row = indexedDF.agg(max(\"userIdIndex\"), max(\"itemIdIndex\")).head\n",
    "    val (userCount, itemCount) = (row.getAs[Double](0).toInt, row.getAs[Double](1).toInt)\n",
    "\n",
    "    val sampleDict = indexedDF.rdd.map(row => row(0) + \",\" + row(1)).collect().toSet\n",
    "    val numberRecords = 1 * indexedDF.count\n",
    "\n",
    "    val ran = new Random(seed = 42L)\n",
    "    val negativeSampleDF = indexedDF.sparkSession.sparkContext\n",
    "      .parallelize(0 to numberRecords.toInt)\n",
    "      .map(x => {\n",
    "        val uid = Math.max(ran.nextInt(userCount), 1)\n",
    "        val iid = Math.max(ran.nextInt(itemCount), 1)\n",
    "        (uid, iid)\n",
    "      })\n",
    "      .filter(x => !sampleDict.contains(x._1 + \",\" + x._2)).distinct()\n",
    "      .map(x => (x._1, x._2, 0.0))\n",
    "      .toDF(\"userIdIndex\", \"itemIdIndex\", \"label\")\n",
    "\n",
    "    indexedDF.union(negativeSampleDF)\n",
    "  }\n",
    "\n",
    "// To tranform the dataframe of sparse features into label points\n",
    "val df2LP: (DataFrame) => DataFrame = df => {\n",
    "    import df.sparkSession.implicits._\n",
    "    df.select(\"userIdIndex\", \"itemIdIndex\", \"label\").rdd.map { r =>\n",
    "      val f = Vectors.dense(r.getDouble(0), r.getDouble(1))\n",
    "      require(f.toArray.take(2).forall(_ >= 0))\n",
    "      val l = r.getDouble(2)\n",
    "      LabeledPoint(l, f)\n",
    "    }.toDF().orderBy(rand()).cache()\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 11:========================================>               (10 + 4) / 14]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val add1 = udf((num: Double) => num + 1)\n",
    "\n",
    "val dataWithNegative = addNegativeSample(indexedDF.withColumn(\"label\", lit(1.0d))).withColumn(\"label\", add1(col(\"label\")))\n",
    "val dataInLP: DataFrame = df2LP(dataWithNegative)\n",
    "\n",
    "val Array(trainingDF, validationDF) = dataInLP.randomSplit(Array(0.8, 0.2), seed = 1L)\n",
    "\n",
    "trainingDF.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we show how to build a Multi-Layer Perceptron (MLP). A ModelParam class is defined to easily change model architecture. \n",
    "* The buttom layer is input layer, then it is embedding layer, which projects the sparse representation to a dense vector. In BigDL, we can use LookUpTable together with Select to create embedding layers, the user (item) embedding has input size of userCount(itemCount), output size of userEmbed (itemEmbed). At last, embedding layers are fed into a multi-layer neural architecture (midLayers). Eventually, a layer of LogSoftMax is added at the end. \n",
    "* Please refer to([BigDL programming guide](https://bigdl-project.github.io/master/#ProgrammingGuide/Model/Functional/)) for more details about functional API, and Nerual Collaborative filtering ([He, 2015](https://www.comp.nus.edu.sg/~xiangnan/papers/ncf.pdf)) for details about the architeture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class ModelParam(userEmbed: Int = 20,\n",
    "                      itemEmbed: Int = 20,\n",
    "                      mfEmbed: Int = 20,\n",
    "                      midLayers: Array[Int] = Array(40, 20, 10),\n",
    "                      labels: Int = 2){\n",
    "  override def toString: String = {\n",
    "    \"userEmbed =\" + userEmbed + \"\\n\" +\n",
    "    \" itemEmbed = \" + itemEmbed + \"\\n\" +\n",
    "    \" mfEmbed = \" + mfEmbed + \"\\n\" +\n",
    "    \" midLayer = \" + midLayers.mkString(\"|\") + \"\\n\" +\n",
    "    \" labels = \" + labels\n",
    "  }\n",
    "}\n",
    "\n",
    "class Model(modelParam: ModelParam) {\n",
    "    import com.intel.analytics.bigdl.nn.Graph.ModuleNode\n",
    "    import com.intel.analytics.bigdl.nn.{Graph, _}\n",
    "    import com.intel.analytics.bigdl.numeric.NumericFloat\n",
    "    import com.intel.analytics.bigdl.tensor.Tensor\n",
    "    import com.intel.analytics.bigdl.nn._\n",
    "    \n",
    "  def this() = {\n",
    "    this(ModelParam())\n",
    "  }\n",
    "\n",
    "  def mlp(userCount: Int, itemCount: Int) = {\n",
    "\n",
    "    println(modelParam )\n",
    "\n",
    "    val input = Identity().inputs()\n",
    "    val select1: ModuleNode[Float] = Select(2, 1).inputs(input)\n",
    "    val select2: ModuleNode[Float] = Select(2, 2).inputs(input)\n",
    "\n",
    "    val userTable = LookupTable(userCount, modelParam.userEmbed)\n",
    "    val itemTable = LookupTable(itemCount, modelParam.itemEmbed)\n",
    "    userTable.setWeightsBias(Array(Tensor[Float](userCount, modelParam.userEmbed).randn(0, 0.1)))\n",
    "    itemTable.setWeightsBias(Array(Tensor[Float](itemCount, modelParam.itemEmbed).randn(0, 0.1)))\n",
    "\n",
    "    val userTableInput = userTable.inputs(select1)\n",
    "    val itemTableInput = itemTable.inputs(select2)\n",
    "\n",
    "    val embeddedLayer = JoinTable(2, 0).inputs(userTableInput, itemTableInput)\n",
    "\n",
    "    val linear1: ModuleNode[Float] = Linear(modelParam.itemEmbed + modelParam.userEmbed,\n",
    "      modelParam.midLayers(0)).inputs(embeddedLayer)\n",
    "\n",
    "    val midLayer = buildMlpModuleNode(linear1, 1, modelParam.midLayers)\n",
    "\n",
    "    val reluLast = ReLU().inputs(midLayer)\n",
    "    val last: ModuleNode[Float] = Linear(modelParam.midLayers.last, modelParam.labels).inputs(reluLast)\n",
    "\n",
    "    val output = if (modelParam.labels >= 2) LogSoftMax().inputs(last) else Sigmoid().inputs(last)\n",
    "\n",
    "    Graph(input, output)\n",
    "  }\n",
    "\n",
    "  private def buildMlpModuleNode(linear: ModuleNode[Float], midLayerIndex: Int, midLayers: Array[Int]): ModuleNode[Float] = {\n",
    "\n",
    "    if (midLayerIndex >= midLayers.length) {\n",
    "      linear\n",
    "    } else {\n",
    "      val relu = ReLU().inputs(linear)\n",
    "      val l = Linear(midLayers(midLayerIndex - 1), midLayers(midLayerIndex)).inputs(relu)\n",
    "      buildMlpModuleNode(l, midLayerIndex + 1, midLayers)\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userEmbed =20\n",
      " itemEmbed = 20\n",
      " mfEmbed = 20\n",
      " midLayer = 20|10\n",
      " labels = 2\n"
     ]
    }
   ],
   "source": [
    "val modelParam = ModelParam(userEmbed = 20,\n",
    "                            itemEmbed = 20,\n",
    "                            midLayers = Array(20,10),\n",
    "                            labels = 2)\n",
    "val recModel = new Model(modelParam)\n",
    "val model = recModel.mlp(userCount.toInt, itemCount.toInt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using DLclassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BigDL provides DLEstimator and DLClassifier for users with Apache Spark MLlib experience, which provides high level API for training a BigDL Model with the Apache Spark Estimator/ Transfomer pattern, thus users can conveniently fit BigDL into a ML pipeline. Please refer to [BigDL guide](https://bigdl-project.github.io/master/#ProgrammingGuide/MLPipeline/#overview) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 14:===================================================>  (189 + 8) / 200]training time(s):  79.983569879\n"
     ]
    }
   ],
   "source": [
    "val criterion = ClassNLLCriterion()\n",
    "val dlc = new DLClassifier(model, criterion, Array(2)).setBatchSize(1000).setOptimMethod(new Adam()).setLearningRate(1e-1).setLearningRateDecay(1e-6).setMaxEpoch(3)\n",
    "\n",
    "val time1 = System.nanoTime()\n",
    "\n",
    "val dlModel: DLModel[Float] = dlc.fit(trainingDF)\n",
    "trainingDF.unpersist()\n",
    "\n",
    "val time2 = System.nanoTime()\n",
    "\n",
    "println(\"training time(s):  \" + (time2-time1)*(1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In recommender systems, for the final the user the most important result is to receive an ordered list of recommendations, from best to worst.  In fact, in many cases the user doesn't care much about the exact ordering of the list - a set of few good recommendations is fine. Therefore, we here we show example of using traditional information retrieval metrics to evaluate the model. Area under the curve, precision and recall are reported. You can implement your own evaluation metrics based on your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.{BinaryClassificationEvaluator, MulticlassClassificationEvaluator}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "object Evaluation {\n",
    "  \n",
    "  def toDecimal(n: Int) = {\n",
    "    (arg: Double) => BigDecimal(arg).setScale(n, BigDecimal.RoundingMode.HALF_UP).toDouble\n",
    "   }\n",
    "    \n",
    "  def evaluate(evaluateDF: DataFrame) = {\n",
    "    val binaryEva = new BinaryClassificationEvaluator().setRawPredictionCol(\"prediction\")\n",
    "    val out1 = binaryEva.evaluate(evaluateDF)\n",
    "    println(\"AUROC: \" + toDecimal(3)(out1))\n",
    "\n",
    "    val multiEva = new MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\")\n",
    "    val out2 = multiEva.evaluate(evaluateDF)\n",
    "    println(\"precision: \" + toDecimal(3)(out2))\n",
    "\n",
    "    val multiEva2 = new MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\")\n",
    "    val out3 = multiEva2.evaluate(evaluateDF)\n",
    "    println(\"recall: \" + toDecimal(3)(out3))\n",
    "\n",
    "    Seq(out1, out2, out3).map(x=> toDecimal(3)(x))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+----------+\n",
      "|label|      features|prediction|\n",
      "+-----+--------------+----------+\n",
      "|  1.0| [40.0,1054.0]|       1.0|\n",
      "|  1.0|  [93.0,910.0]|       2.0|\n",
      "|  1.0|[142.0,2511.0]|       1.0|\n",
      "+-----+--------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "[Stage 21645:=================================================> (194 + 6) / 200]AUROC: 0.713\n",
      "precision: 0.908\n",
      "recall: 0.918\n",
      "userEmbed =20\n",
      " itemEmbed = 20\n",
      " mfEmbed = 20\n",
      " midLayer = 20|10\n",
      " labels = 2\n",
      "0.713 | 0.908 | 0.918\n",
      "\n",
      "prediction and evaluation time(s):  12.622876880000002\n"
     ]
    }
   ],
   "source": [
    "val predictions = dlModel.setBatchSize(1).transform(validationDF)\n",
    "predictions.show(3)\n",
    "predictions.cache()\n",
    "\n",
    "val toZero = udf { d: Double =>\n",
    "    if (d > 1) 1.0 else 0.0\n",
    " }\n",
    "val res = Evaluation.evaluate(predictions.withColumn(\"label\", toZero(col(\"label\"))).withColumn(\"prediction\", toZero(col(\"prediction\"))))\n",
    "val time3 = System.nanoTime()\n",
    "val resStr = modelParam +\"\\n\" + res.mkString(\" | \") +\"\\n\"\n",
    "\n",
    "println(resStr)\n",
    "println(\"prediction and evaluation time(s):  \" + (time3-time2)*(1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example to show recommendations for users, recommendations for item. \n",
    "* recommendForUsers is defined to return a DataFrame of (userIdIndex: Int, recommendations), where recommendations are stored as an array of (itemIdIndex: Int, rating: Float) Rows.\n",
    "* recommendForItems is defined to return a DataFrame of (itemIdIndex: Int, recommendations), where recommendations are stored as an array of (userIdIndex: Int, rating: Float) Rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "case class featuresScore(feature1: Int, feature2: Int, score: Double)\n",
    "def recommend(predictions: DataFrame, numItems: Int, forUser: Boolean = true): DataFrame = {\n",
    "    import predictions.sqlContext.implicits._\n",
    "    predictions.rdd.map(row =>\n",
    "      if (forUser) {\n",
    "        featuresScore(row.getAs[DenseVector](1)(0).toInt,\n",
    "          row.getAs[DenseVector](1)(1).toInt, row.getDouble(2))\n",
    "      } else {\n",
    "        featuresScore(row.getAs[DenseVector](1)(1).toInt,\n",
    "          row.getAs[DenseVector](1)(0).toInt, row.getDouble(2))\n",
    "      })\n",
    "      .groupBy(x => x.feature1)\n",
    "      .map(x => (x._1, x._2.toList.sortBy(x => x.score).reverse.map(x => (x.feature2, x.score)).take(numItems)))\n",
    "      .toDF(\"id\", \"recommendations\")\n",
    "  }\n",
    "\n",
    "def rankForNegative(usersForRec: DataFrame, itemsForRec: DataFrame, indexed: DataFrame, dlModel: DLModel[Float]) = {\n",
    "    val all = usersForRec.select(\"userIdIndex\").join(itemsForRec.select(\"itemIdIndex\"))\n",
    "      .except(indexed.select(\"userIdIndex\", \"itemIdIndex\"))\n",
    "    dlModel.transform(df2LP(all.withColumn(\"label\", lit(0.0d))))\n",
    "  }\n",
    "  \n",
    "/**\n",
    "    * Returns top `numItems`  recommended for each user, for usersForRec users.\n",
    "    *\n",
    "    * @param usersForRec a dataframe of users who want to be recommended\n",
    "    * @param itemsForRec a dataframe of items\n",
    "    * @param indexed     original data input\n",
    "    * @param dlModel     a trained deep learning model\n",
    "    * @param numItems    max number of recommendations for each item\n",
    "    * @return a DataFrame of (userIdIndex: Int, recommendations), where recommendations are\n",
    "    *         stored as an array of (itemIdIndex: Int, rating: Float) Rows.\n",
    "    */\n",
    "  def recommendForUsers(usersForRec: DataFrame,\n",
    "                        itemsForRec: DataFrame,\n",
    "                        indexed: DataFrame,\n",
    "                        dlModel: DLModel[Float],\n",
    "                        numItems: Int): DataFrame = {\n",
    "    val predictions = rankForNegative(usersForRec, itemsForRec, indexed, dlModel)\n",
    "\n",
    "    recommend(predictions, numItems, true)\n",
    "  }\n",
    " /**\n",
    "    * Returns top `numUsers` users recommended for each item, for itemsForRec items.\n",
    "    *\n",
    "    * @param usersForRec a dataframe of users who want to be recommended\n",
    "    * @param itemsForRec a dataframe of items\n",
    "    * @param indexed     original data input\n",
    "    * @param dlModel     a trained deep learning model\n",
    "    * @param numUsers    max number of recommendations for each item\n",
    "    * @return a DataFrame of (itemIdIndex: Int, recommendations), where recommendations are\n",
    "    *         stored as an array of (userIdIndex: Int, rating: Float) Rows.\n",
    "    */\n",
    "  def recommendForItems(usersForRec: DataFrame,\n",
    "                        itemsForRec: DataFrame,\n",
    "                        indexed: DataFrame,\n",
    "                        dlModel: DLModel[Float],\n",
    "                        numUsers: Int): DataFrame = {\n",
    "    val predictions = rankForNegative(usersForRec, itemsForRec, indexed, dlModel)\n",
    "\n",
    "    recommend(predictions, numUsers, false)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 21688:=============================================>     (179 + 8) / 200]root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _1: integer (nullable = false)\n",
      " |    |    |-- _2: double (nullable = false)\n",
      "\n",
      "[Stage 21695:=================================================> (195 + 5) / 200]+----+------------------------------------+\n",
      "|id  |recommendations                     |\n",
      "+----+------------------------------------+\n",
      "|1051|[[2715,2.0], [1901,2.0], [687,2.0]] |\n",
      "|692 |[[489,2.0], [170,2.0], [3155,2.0]]  |\n",
      "|496 |[[1513,2.0], [1525,2.0], [2651,2.0]]|\n",
      "|299 |[[1031,2.0], [2448,2.0], [2666,2.0]]|\n",
      "|305 |[[650,2.0], [413,2.0], [2630,2.0]]  |\n",
      "|934 |[[2193,2.0], [146,2.0], [1181,2.0]] |\n",
      "|558 |[[3113,2.0], [2669,2.0], [3076,2.0]]|\n",
      "|1761|[[920,2.0], [3313,2.0], [2443,2.0]] |\n",
      "|769 |[[1037,2.0], [3200,2.0], [2770,2.0]]|\n",
      "|596 |[[1777,2.0], [3152,2.0], [246,2.0]] |\n",
      "+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val usersForRec = indexedDF.select(\"userIdIndex\").distinct.limit(10)\n",
    "val userRec = recommendForUsers(usersForRec, indexedDF.select(\"itemIdIndex\").distinct(), indexedDF, dlModel, 3)\n",
    "userRec.printSchema()\n",
    "userRec.show(false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage 21710:==============================================>    (181 + 8) / 200]root\n",
      " |-- id: integer (nullable = false)\n",
      " |-- recommendations: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- _1: integer (nullable = false)\n",
      " |    |    |-- _2: double (nullable = false)\n",
      "\n",
      "[Stage 21717:==================================================>(199 + 1) / 200]+----+------------------------------------+\n",
      "|id  |recommendations                     |\n",
      "+----+------------------------------------+\n",
      "|2815|[[2641,2.0], [4152,2.0], [5715,2.0]]|\n",
      "|1051|[[1725,2.0], [3011,2.0], [5941,2.0]]|\n",
      "|692 |[[731,2.0], [5340,2.0], [1983,2.0]] |\n",
      "|496 |[[2685,2.0], [3967,2.0], [4079,2.0]]|\n",
      "|299 |[[1440,2.0], [2908,2.0], [4285,2.0]]|\n",
      "|305 |[[310,2.0], [4731,2.0], [4433,2.0]] |\n",
      "|2734|[[381,2.0], [1795,2.0], [3144,2.0]] |\n",
      "|934 |[[252,2.0], [143,2.0], [2478,2.0]]  |\n",
      "|596 |[[4419,2.0], [5123,2.0], [5660,2.0]]|\n",
      "|3597|[[3621,2.0], [5722,2.0], [2870,2.0]]|\n",
      "+----+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val itemsForRec = indexedDF.select(\"itemIdIndex\").distinct.limit(10)\n",
    "val itemRec = recommendForItems(indexedDF.select(\"userIdIndex\").distinct(), itemsForRec, indexedDF, dlModel, 3)\n",
    "itemRec.printSchema()\n",
    "itemRec.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "val x  =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "file_extension": ".scala",
   "name": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
