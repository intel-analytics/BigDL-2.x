{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image classification using tensorflow pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow-Slim image classification model library provides both the implementation and pre-trianed checkpoint many popular convolution nerual nets for image classification.\n",
    "\n",
    "Using TFNet in Analytics-Zoo, we can easily load these pre-trained model and make distributed inference with only a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the slim image classification model library to $PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.util.common import *\n",
    "init_engine()\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path\n",
    "slim_path = \"/home/ludviq/workspace/models/research/slim\" # Please set this to the directory where you clone the tensorflow models repository\n",
    "sys.path.append(slim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the inference graph and restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ludviq/workspace/checkpoint/inception_v1.ckpt\n"
     ]
    }
   ],
   "source": [
    "from datasets import dataset_factory\n",
    "from nets import inception\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image = tf.random_uniform(shape=[224, 224, 3])\n",
    "batch_size_tensor = tf.placeholder_with_default(32, shape=())\n",
    "images = tf.train.batch(\n",
    "    [image],\n",
    "    batch_size=batch_size_tensor,\n",
    "    num_threads=4,\n",
    "    capacity=5 * 32)\n",
    "# One thing to note is that the batch_size must be placeholder, so that batch size is not hard coded into graph and analytics-zoo can work on different batch sizes.\n",
    "# The above code can be replaced with a single line of code\n",
    "# images = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3))\n",
    "\n",
    "with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "    logits, end_points = inception.inception_v1(images, num_classes=1001, is_training=False)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/home/ludviq/workspace/checkpoint/inception_v1.ckpt\") # You need to edit this path to the checkpoint you downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the graph as a frozen inference graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export_tf utility function will frozen the tensorflow graph, strip unused operation according to the inputs and outputs and save it to the specified directory along with the input/output tensor names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 230 variables.\n",
      "Converted 230 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from zoo.util.tf import export_tf\n",
    "export_tf(sess, \"/home/ludviq/workspace/models/tfnet/\", inputs=[images], outputs=[logits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to Analytics-Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTFNet\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.net import TFNet\n",
    "model = TFNet.from_export_folder(\"/home/ludviq/workspace/models/tfnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it on one image\n",
    "\n",
    "![Test Image](./test.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "im = cv2.imread(\"test.jpg\")\n",
    "im = cv2.resize(im, (224, 224))\n",
    "im = (im - 127.0) / 128.0\n",
    "im = np.expand_dims(im, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"imagenet_class_index.json\") as f:\n",
    "    class_idx = json.load(f)\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persian_cat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "result = model.predict([im])\n",
    "print(idx2label[np.argmax(result, 1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fine-tune to train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the inference graph and restore the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/ludviq/workspace/checkpoint/inception_v1.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()# if you want to test your code, you can use it to reset your graph and to avoid some mistakes\n",
    "\n",
    "image = tf.random_uniform(shape=[224, 224, 3])\n",
    "batch_size_tensor = tf.placeholder_with_default(32, shape=())\n",
    "images = tf.train.batch(\n",
    "    [image],\n",
    "    batch_size=batch_size_tensor,\n",
    "    num_threads=4,\n",
    "    capacity=5 * 32)\n",
    "# One thing to note is that the batch_size must be placeholder, so that batch size is not hard coded into graph and analytics-zoo can work on different batch sizes.\n",
    "# The above code can be replaced with a single line of code\n",
    "#images = tf.placeholder(dtype=tf.float32, shape=(None, 224, 224, 3))\n",
    "\n",
    "with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "    logits, end_points = inception.inception_v1(images, num_classes=1001)\n",
    "\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"/home/ludviq/workspace/checkpoint/inception_v1.ckpt\")# You need to edit this path to the checkpoint you downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the graph you want as a frozen inference graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all the end_points, and find the end_points that you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MaxPool_3a_3x3': <tf.Tensor 'InceptionV1/InceptionV1/MaxPool_3a_3x3/MaxPool:0' shape=(?, 28, 28, 192) dtype=float32>, 'MaxPool_2a_3x3': <tf.Tensor 'InceptionV1/InceptionV1/MaxPool_2a_3x3/MaxPool:0' shape=(?, 56, 56, 64) dtype=float32>, 'Predictions': <tf.Tensor 'InceptionV1/Logits/Predictions/Reshape_1:0' shape=(?, 1001) dtype=float32>, 'Conv2d_1a_7x7': <tf.Tensor 'InceptionV1/InceptionV1/Conv2d_1a_7x7/Relu:0' shape=(?, 112, 112, 64) dtype=float32>, 'Mixed_4d': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_4d/concat:0' shape=(?, 14, 14, 512) dtype=float32>, 'MaxPool_4a_3x3': <tf.Tensor 'InceptionV1/InceptionV1/MaxPool_4a_3x3/MaxPool:0' shape=(?, 14, 14, 480) dtype=float32>, 'Mixed_3c': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_3c/concat:0' shape=(?, 28, 28, 480) dtype=float32>, 'Mixed_3b': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_3b/concat:0' shape=(?, 28, 28, 256) dtype=float32>, 'Mixed_5c': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_5c/concat:0' shape=(?, 7, 7, 1024) dtype=float32>, 'Mixed_5b': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_5b/concat:0' shape=(?, 7, 7, 832) dtype=float32>, 'Mixed_4b': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_4b/concat:0' shape=(?, 14, 14, 512) dtype=float32>, 'Mixed_4c': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_4c/concat:0' shape=(?, 14, 14, 512) dtype=float32>, 'Conv2d_2c_3x3': <tf.Tensor 'InceptionV1/InceptionV1/Conv2d_2c_3x3/Relu:0' shape=(?, 56, 56, 192) dtype=float32>, 'Conv2d_2b_1x1': <tf.Tensor 'InceptionV1/InceptionV1/Conv2d_2b_1x1/Relu:0' shape=(?, 56, 56, 64) dtype=float32>, 'Mixed_4e': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_4e/concat:0' shape=(?, 14, 14, 528) dtype=float32>, 'Logits': <tf.Tensor 'InceptionV1/Logits/SpatialSqueeze:0' shape=(?, 1001) dtype=float32>, 'AvgPool_0a_7x7': <tf.Tensor 'InceptionV1/Logits/AvgPool_0a_7x7/AvgPool:0' shape=(?, 1, 1, 1024) dtype=float32>, 'Mixed_4f': <tf.Tensor 'InceptionV1/InceptionV1/Mixed_4f/concat:0' shape=(?, 14, 14, 832) dtype=float32>, 'MaxPool_5a_2x2': <tf.Tensor 'InceptionV1/InceptionV1/MaxPool_5a_2x2/MaxPool:0' shape=(?, 7, 7, 832) dtype=float32>}\n"
     ]
    }
   ],
   "source": [
    "print end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The export_tf utility function will frozen the tensorflow graph, strip unused operation according to the inputs and outputs and save it to the specified directory along with the input/output tensor names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 114 variables.\n",
      "Converted 114 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "from zoo.util.tf import export_tf\n",
    "avg_pool = end_points['AvgPool_0a_7x7']\n",
    "export_tf(sess, \"/home/ludviq/workspace/models/tfnet/\", inputs=[images], outputs=[avg_pool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load to Analytics-Zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load that your freezed model from the specific directory above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createTFNet\n"
     ]
    }
   ],
   "source": [
    "from zoo.pipeline.api.net import TFNet\n",
    "amodel = TFNet.from_export_folder(\"/home/ludviq/workspace/models/tfnet/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sequential model to build your model. First transport the type of data from NCHW to NHWC. Then multiply the scala and make the input and output both contiguous. Add the linear layer to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createSequential\n",
      "creating: createTranspose\n",
      "creating: createMulConstant\n",
      "creating: createContiguous\n",
      "creating: createView\n",
      "creating: createLinear\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bigdl.nn.layer.Sequential at 0x7ff6d9f09b50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bigdl.nn.layer import Sequential,Transpose,Contiguous,Linear,ReLU, SoftMax, Reshape, View, MulConstant, SpatialAveragePooling\n",
    "full_model = Sequential()\n",
    "full_model.add(Transpose([(2,4), (2,3)]))\n",
    "scala = 1. /255\n",
    "full_model.add(MulConstant(scala))\n",
    "full_model.add(Contiguous())\n",
    "full_model.add(amodel)\n",
    "full_model.add(View([1024]))\n",
    "full_model.add(Linear(1024,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to the cat_dog data you downloaded. Then read these images from this path.\n",
    "Use udf functions to design some functions to mark data.\n",
    "Seperate the data into two groups: training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createDefault\n",
      "creating: createSGD\n",
      "creating: createSeqToTensor\n",
      "creating: createSeqToTensor\n",
      "creating: createSeqToTensor\n",
      "creating: createSeqToTensor\n",
      "creating: createSeqToTensor\n",
      "+------------+-----+\n",
      "|        name|label|\n",
      "+------------+-----+\n",
      "|cat.4304.jpg|  1.0|\n",
      "|cat.8362.jpg|  1.0|\n",
      "| cat.711.jpg|  1.0|\n",
      "|cat.4361.jpg|  1.0|\n",
      "|cat.1491.jpg|  1.0|\n",
      "|cat.4931.jpg|  1.0|\n",
      "|cat.5675.jpg|  1.0|\n",
      "|dog.1551.jpg|  2.0|\n",
      "|cat.7118.jpg|  1.0|\n",
      "|dog.1495.jpg|  2.0|\n",
      "+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bigdl.nn.criterion import CrossEntropyCriterion\n",
    "from pyspark import SparkConf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from zoo.common.nncontext import *\n",
    "from zoo.feature.image import *\n",
    "from zoo.pipeline.api.keras.layers import Dense, Input, Flatten\n",
    "from zoo.pipeline.api.keras.models import *\n",
    "from zoo.pipeline.api.net import *\n",
    "from zoo.pipeline.nnframes import *\n",
    "image_path = \"file:///home/ludviq/cat_dog/minitrain/*.jpg\"\n",
    "imageDF = NNImageReader.readImages(image_path, sc)\n",
    "\n",
    "getName = udf(lambda row:\n",
    "                  re.search(r'(cat|dog)\\.([\\d]*)\\.jpg', row[0], re.IGNORECASE).group(0),\n",
    "                  StringType())\n",
    "getLabel = udf(lambda name: 1.0 if name.startswith('cat') else 2.0, DoubleType())\n",
    "\n",
    "labelDF = imageDF.withColumn(\"name\", getName(col(\"image\"))) \\\n",
    "        .withColumn(\"label\", getLabel(col('name')))\n",
    "(trainingDF, validationDF) = labelDF.randomSplit([0.9, 0.1])\n",
    "labelDF.select(\"name\",\"label\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the distribution of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  1.0|   58|\n",
      "|  2.0|   59|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingDF.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pipeline to train the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the preprocessing processes together. Before identify the classifier, design the summary's name to make sure we will store the log in right name. Identify the args of classifier. Then save logs to the dirctionary. Use the classifier identified just to create pipeline. Use this pipeline as your model to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating: createRowToImageFeature\n",
      "creating: createImageResize\n",
      "creating: createImageChannelNormalize\n",
      "creating: createImageMatToTensor\n",
      "creating: createImageFeatureToTensor\n",
      "creating: createChainedPreprocessing\n",
      "creating: createTrainSummary\n",
      "creating: createCrossEntropyCriterion\n",
      "creating: createScalarToTensor\n",
      "creating: createFeatureLabelPreprocessing\n",
      "creating: createNNClassifier\n",
      "('Saving logs to ', 'classification cat vs dog20180726-085804')\n"
     ]
    }
   ],
   "source": [
    "transformer = ChainedPreprocessing(\n",
    "        [RowToImageFeature(), ImageResize(224, 224),\n",
    "         ImageChannelNormalize(123.0, 117.0, 104.0), ImageMatToTensor(), ImageFeatureToTensor()])\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "import datetime as dt\n",
    "app_name='classification cat vs dog'+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_summary = TrainSummary(log_dir='./log/',\n",
    "                                     app_name=app_name)\n",
    "classifier = NNClassifier(full_model, CrossEntropyCriterion(), transformer)\\\n",
    "        .setFeaturesCol(\"image\")\\\n",
    "        .setLearningRate(0.003)\\\n",
    "        .setBatchSize(16)\\\n",
    "        .setMaxEpoch(9)\\\n",
    "        .setTrainSummary(train_summary)\n",
    "# BatchSize is a multiple of physcial_core_number(in your bash command)\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "import os\n",
    "import datetime as dt\n",
    "if not os.path.exists(\"./log\"):\n",
    "    os.makedirs(\"./log\")\n",
    "    \n",
    "print(\"Saving logs to \", app_name)\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "trainedModel = pipeline.fit(trainingDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model on validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the vallidation data to test the model, and calculate the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the transform data\n",
    "predictionDF = trainedModel.transform(validationDF).cache()\n",
    "# caculate the correct rate and the test error\n",
    "correct = predictionDF.filter(\"label=prediction\").count()\n",
    "overall = predictionDF.count()\n",
    "accuracy = correct * 1.0 / overall\n",
    "\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the pic can be seen on this page. Repeatedly run this code will result in a warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a pic about the result and make it can show in GUI\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "%pylab inline\n",
    "\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the change in loss using a pic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read loss from summary nad show in pic\n",
    "loss = np.array(train_summary.read_scalar(\"Loss\"))\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.plot(loss[:,0], loss[:,1], label='loss')\n",
    "plt.xlim(0, loss.shape[0]+10)\n",
    "plt.grid(True)\n",
    "plt.title(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect two examples from both sides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplecat=predictionDF.filter(predictionDF.prediction==1.0).limit(2).collect()\n",
    "sampledog=predictionDF.filter(predictionDF.prediction==2.0).sort(\"label\", ascending=False).limit(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show two cat pics and their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "for cat in samplecat:\n",
    "    print (\"prediction:\"), cat.prediction\n",
    "    display(Image(cat.image.origin[5:], height=256,width=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show two dog pics and their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the two pics and their things as follow\n",
    "from IPython.display import Image, display\n",
    "for dog in sampledog:\n",
    "    print (\"prediction:\"), dog.prediction\n",
    "    \n",
    "    display(Image(dog.image.origin[5:], height=256,width=256))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
